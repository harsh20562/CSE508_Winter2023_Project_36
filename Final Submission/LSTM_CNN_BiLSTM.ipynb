{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import keras.utils\n",
        "from keras.callbacks import TensorBoard, CSVLogger\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dense,Flatten,LSTM,Conv1D,GlobalMaxPool1D,Dropout,Bidirectional\n",
        "from keras import optimizers\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.utils import pad_sequences\n",
        "from keras.models import load_model\n",
        "\n",
        "import pandas as pd\n",
        "import os.path\n",
        "import pickle \n",
        "import numpy as np\n",
        "from IPython.display import SVG\n",
        "import time\n",
        "import re\n",
        "import glob\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "import spacy\n",
        "!python -m spacy download en\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "import operator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9MrTpv9O2bj",
        "outputId": "8d12adfe-8c80-466a-9b7b-9d95c8bb79c1"
      },
      "execution_count": 425,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-23 18:03:56.874325: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
            "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.9/dist-packages (from en-core-web-sm==3.5.0) (3.5.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.6.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qIIlSB4O6vk",
        "outputId": "fd6e147a-22cd-4c2c-d1ff-2207a8cbb380"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/drive/MyDrive/Datasets Sem-6/IR Datasets/Project/liar_dataset.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pD1Y3qCMPDQY",
        "outputId": "bb0d342e-c575-4629-e95f-b1ea6f64b8d7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Datasets Sem-6/IR Datasets/Project/liar_dataset.zip\n",
            "  inflating: README                  \n",
            "  inflating: test.tsv                \n",
            "  inflating: train.tsv               \n",
            "  inflating: valid.tsv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('/content/train.tsv',sep='\\t', header = None)\n",
        "test_df = pd.read_csv('/content/test.tsv',sep='\\t', header = None)\n",
        "val_df = pd.read_csv('/content/valid.tsv',sep='\\t', header = None)"
      ],
      "metadata": {
        "id": "VrnmWzSCPGRS"
      },
      "execution_count": 433,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.drop([0, 8, 9, 10, 11, 12], axis = 1)\n",
        "test_df = test_df.drop([0, 8, 9, 10, 11, 12], axis = 1)\n",
        "val_df = val_df.drop([0, 8, 9, 10, 11, 12], axis = 1)\n",
        "\n",
        "train_df.columns = ['label', 'statement', 'subject', 'speaker', 'speaker job title', 'state info', 'party affiliation', 'location of statement']\n",
        "test_df.columns = ['label', 'statement', 'subject', 'speaker', 'speaker job title', 'state info', 'party affiliation', 'location of statement']\n",
        "val_df.columns = ['label', 'statement', 'subject', 'speaker', 'speaker job title', 'state info', 'party affiliation', 'location of statement']"
      ],
      "metadata": {
        "id": "vCo5454ePIfR"
      },
      "execution_count": 434,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.dropna()\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "test_df = test_df.dropna()\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "val_df = val_df.dropna()\n",
        "val_df = val_df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "7op4eR8RPUki"
      },
      "execution_count": 435,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.concat([train_df, val_df])\n",
        "train_df = train_df.reset_index()\n",
        "train_df = train_df.drop(['index'], axis = 1)"
      ],
      "metadata": {
        "id": "IMr9_UsKPaa4"
      },
      "execution_count": 436,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_dict = {'mostly-true':4,'barely-true':2,'half-true':3,'false':1, 'true':5,'pants-fire':0}\n",
        "train_df['label'] = train_df['label'].apply(lambda x: labels_dict[x])\n",
        "test_df['label'] = test_df['label'].apply(lambda x: labels_dict[x])"
      ],
      "metadata": {
        "id": "1XaGztd1Pc6O"
      },
      "execution_count": 437,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['speaker_enc'] = label_encoder.fit_transform(train_df['speaker'])\n",
        "train_df['state info_enc'] = label_encoder.fit_transform(train_df['state info'])\n",
        "train_df['speaker job title_enc'] = label_encoder.fit_transform(train_df['speaker job title'])\n",
        "train_df['party affiliation_enc'] = label_encoder.fit_transform(train_df['party affiliation'])\n",
        "train_df['subject_enc'] = label_encoder.fit_transform(train_df['subject'])\n",
        "\n",
        "test_df['speaker_enc'] = label_encoder.fit_transform(test_df['speaker'])\n",
        "test_df['state info_enc'] = label_encoder.fit_transform(test_df['state info'])\n",
        "test_df['speaker job title_enc'] = label_encoder.fit_transform(test_df['speaker job title'])\n",
        "test_df['party affiliation_enc'] = label_encoder.fit_transform(test_df['party affiliation'])\n",
        "test_df['subject_enc'] = label_encoder.fit_transform(test_df['subject'])"
      ],
      "metadata": {
        "id": "pG_JFkQwiXdz"
      },
      "execution_count": 438,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "  text = text.lower() # lower - casing the text\n",
        "  text = re.sub('<[^>]*>', ' ', text)\n",
        "  text = re.sub('[\\W]+', ' ', text)\n",
        "  tokenizer = TreebankWordTokenizer()\n",
        "  words = tokenizer.tokenize(text)\n",
        "  text = ' '.join(words)\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  word_tokens = word_tokenize(text)\n",
        "  filtered_sentence = [w for w in word_tokens if not w in stop_words] # removal of stopwords\n",
        "  text = ' '.join(filtered_sentence)\n",
        "  text = text_to_word_sequence(text)\n",
        "  val = [0] * 10\n",
        "  val = [vocab_dict[t] for t in text if t in vocab_dict]\n",
        "  return val\n",
        "\n",
        "def get_vocab_dict(train_data):\n",
        "  vocab_dict = {}\n",
        "  if not os.path.exists('vocabulary.p'):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(train_data['statement'])\n",
        "    vocab_dict = tokenizer.word_index\n",
        "    pickle.dump(vocab_dict, open( \"vocabulary.p\", \"wb\" ))\n",
        "  else:\n",
        "    vocab_dict = pickle.load(open(\"vocabulary.p\", \"rb\" ))\n",
        "  return vocab_dict"
      ],
      "metadata": {
        "id": "9A8TtsZZP7be"
      },
      "execution_count": 439,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_dict = get_vocab_dict(train_df)"
      ],
      "metadata": {
        "id": "BnQJdd6IkuR_"
      },
      "execution_count": 440,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['statement_id'] = train_df['statement'].apply(preprocess)\n",
        "test_df['statement_id'] = test_df['statement'].apply(preprocess)"
      ],
      "metadata": {
        "id": "zUloXgXUk-_I"
      },
      "execution_count": 441,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tags = {'ADJ': 'adjective', 'ADP': 'adposition', 'ADV': 'adverb', \n",
        "            'AUX': 'auxiliary verb', 'CONJ': 'coordinating conjunction', \n",
        "            'DET': 'determiner', 'INTJ': 'interjection', 'NOUN': 'noun', \n",
        "            'NUM': 'numeral', 'PART': 'particle', 'PRON': 'pronoun', \n",
        "            'PROPN': 'proper noun', 'PUNCT': 'punctuation', 'X': 'other', \n",
        "            'SCONJ': 'subord conjunction', 'SYM': 'symbol', 'VERB': 'verb'}\n",
        "\n",
        "pos_dict = {'NOUN' : 0, 'VERB' : 1, 'ADP' : 2, 'PROPN' : 3, 'PUNCT' : 4, \n",
        "            'DET' : 5, 'ADJ' : 6, 'NUM' : 7, 'ADV' : 8, 'PRON' : 9, 'X' : 9, \n",
        "            'PART' : 9, 'SYM' : 9, 'INTJ' : 9 }\n",
        "\n",
        "def get_pos(statement):\n",
        "  doc = nlp(statement)\n",
        "  taglist = []\n",
        "  for token in doc:\n",
        "    taglist.append(pos_dict.get(token.pos_,max(pos_dict.values())))\n",
        "  return taglist"
      ],
      "metadata": {
        "id": "vhmOZ4EumJpD"
      },
      "execution_count": 442,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['pos_id'] = train_df['statement'].apply(get_pos)\n",
        "test_df['pos_id'] = test_df['statement'].apply(get_pos)"
      ],
      "metadata": {
        "id": "NxjSoFqVmi4G"
      },
      "execution_count": 443,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a dictionary for storing the glove embeddings in the dictionary\n",
        "glove_path = '/content/drive/MyDrive/glove.6B.100d.txt'\n",
        "embeddings = {}\n",
        "file1 = open(glove_path)\n",
        "for i in file1:\n",
        "  line = i.split()\n",
        "  vector = np.asarray(line[1:], dtype = 'float32')\n",
        "  embeddings[line[0]] = vector\n",
        "file1.close()\n",
        "\n",
        "emb_dimension = 100 # Embedding dimensions obtained from glove dictionary\n",
        "\n",
        "\n",
        "# Creating Embedding matrix for the given vocabluary\n",
        "num_words = len(vocab_dict) + 1\n",
        "emb_matrix = np.zeros((num_words, emb_dimension))\n",
        "for word, i in vocab_dict.items():\n",
        "    emb_vector = embeddings.get(word)\n",
        "    if emb_vector is not None:\n",
        "        emb_matrix[i] = emb_vector\n",
        "\n",
        "# Creating POS embedding matrix of size having maximum pos embeddings in a statement\n",
        "emb_index = None\n",
        "pos_embeddings = np.identity(max(pos_dict.values()), dtype=int)"
      ],
      "metadata": {
        "id": "aZfwyp88nJax"
      },
      "execution_count": 444,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train_df['statement_id']\n",
        "y_train = train_df['label']\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes=6)\n",
        "X_test = test_df['statement_id']\n",
        "y_test = test_df['label']\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes=6)\n",
        "X_train_pos = train_df['pos_id']\n",
        "X_test_pos = test_df['pos_id']"
      ],
      "metadata": {
        "id": "2_9OkqVGotRu"
      },
      "execution_count": 445,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_length = len(vocab_dict.keys())\n",
        "hidden_size = emb_dimension \n",
        "lstm_size = 100\n",
        "num_steps = 15\n",
        "num_epochs = 30\n",
        "batch_size = 40\n",
        "kernel_sizes = [3,3]\n",
        "filter_size = 128"
      ],
      "metadata": {
        "id": "5x9bUb2wwUSH"
      },
      "execution_count": 446,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pad_sequences(X_train, maxlen=num_steps, padding='post',truncating='post')\n",
        "X_test = pad_sequences(X_test, maxlen=num_steps, padding='post',truncating='post')\n",
        "X_train_pos = pad_sequences(X_train_pos, maxlen=num_steps, padding='post',truncating='post')\n",
        "X_test_pos = pad_sequences(X_test_pos, maxlen=num_steps, padding='post',truncating='post')"
      ],
      "metadata": {
        "id": "yh1A6c4zpoQF"
      },
      "execution_count": 447,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training on statement with and without POS taggings data**"
      ],
      "metadata": {
        "id": "ZUVCD0mT8Ka_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "use_pos=False\n",
        "def train(model, name, use_pos=False):\n",
        "  sgd = optimizers.SGD(lr=0.25, clipvalue=0.4, nesterov=True)\n",
        "  adam = optimizers.Adam(lr=0.00075, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "  model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['categorical_accuracy'],)\n",
        "  tb = TensorBoard()\n",
        "         \n",
        "  if use_pos:\n",
        "    model.fit({'main_input': X_train, 'pos_input': X_train_pos}, {'main_output': y_train}, epochs = num_epochs, batch_size = batch_size)\n",
        "  else:\n",
        "    model.fit({'main_input': X_train}, {'main_output': y_train}, epochs = num_epochs, batch_size = batch_size)\n",
        "  predictions = predict_test(model,name,use_pos)\n",
        "  return predictions\n",
        "      \n",
        "def predict_test(model, name, use_pos=False):   \n",
        "  preds = []\n",
        "  if use_pos:\n",
        "    preds = model.predict([X_test], batch_size=batch_size, verbose=1)\n",
        "\n",
        "  else:\n",
        "    preds = model.predict([X_test], batch_size=batch_size, verbose=1)\n",
        "\n",
        "  predictions = np.array([np.argmax(pred) for pred in preds])\n",
        "  return predictions\n",
        "\n",
        "\n",
        "def predict_train(model, name, use_pos=False):\n",
        "  preds = []\n",
        "  if use_pos:\n",
        "    preds = model.predict([X_train], batch_size=batch_size, verbose=1)\n",
        "\n",
        "  else:\n",
        "    preds = model.predict([X_train], batch_size=batch_size, verbose=1)  \n",
        "  \n",
        "  predictions = np.array([np.argmax(pred) for pred in preds])\n",
        "  return predictions\n"
      ],
      "metadata": {
        "id": "7nmyQyxDr3hG"
      },
      "execution_count": 448,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CNN Model**"
      ],
      "metadata": {
        "id": "tUC1VSeXFC4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filter_without_pos = []\n",
        "filter_with_pos = []\n",
        "statement_input = Input(shape=(num_steps,), dtype='int32', name='main_input')\n",
        "x_stmt = Embedding(vocab_length+1,emb_dimension,weights=[emb_matrix],input_length=num_steps,trainable=False)(statement_input)\n",
        "pos_input = Input(shape=(num_steps,), dtype='int32', name='pos_input')\n",
        "x_pos = Embedding(max(pos_dict.values()), max(pos_dict.values()), weights=[pos_embeddings], input_length=num_steps, trainable=False)(pos_input)"
      ],
      "metadata": {
        "id": "DowoGv2sssdZ"
      },
      "execution_count": 471,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for kernel in kernel_sizes:\n",
        "  x_1 = Conv1D(filters=filter_size,kernel_size=kernel)(x_stmt)\n",
        "  x_1 = GlobalMaxPool1D()(x_1)\n",
        "  filter_without_pos.append(x_1)\n",
        "    \n",
        "  x_2 = Conv1D(filters=filter_size,kernel_size=kernel)(x_pos)\n",
        "  x_2 = GlobalMaxPool1D()(x_2)\n",
        "  filter_with_pos.append(x_2)\n",
        "\n",
        "conv_in1 = keras.layers.concatenate(filter_without_pos)\n",
        "conv_in1 = Dropout(0.2)(conv_in1)\n",
        "conv_in1 = Dense(128, activation='relu')(conv_in1)\n",
        "conv_in2 = keras.layers.concatenate(filter_with_pos)\n",
        "conv_in2 = Dropout(0.2)(conv_in2)\n",
        "conv_in2 = Dense(128, activation='relu')(conv_in2)\n",
        "\n",
        "x = conv_in1\n",
        "if use_pos:\n",
        "  x = keras.layers.concatenate([conv_in1, conv_in2])\n",
        "else:\n",
        "  x = conv_in1\n",
        "\n",
        "output = Dense(6, activation='softmax', name='main_output')(x)\n",
        "\n",
        "if use_pos:\n",
        "  model_cnn = Model(inputs=[statement_input, pos_input], outputs=[output])\n",
        "else:\n",
        "  model_cnn = Model(inputs=[statement_input], outputs=[output])"
      ],
      "metadata": {
        "id": "wDpEXqWdt7jk"
      },
      "execution_count": 472,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = train(model_cnn,'cnn',use_pos=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBSP6mBAvBkD",
        "outputId": "ee4ee413-dcf2-4a67-cd1c-7f47e6da3f8c"
      },
      "execution_count": 451,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "190/190 [==============================] - 2s 8ms/step - loss: 1.7709 - categorical_accuracy: 0.2169\n",
            "Epoch 2/30\n",
            "190/190 [==============================] - 1s 8ms/step - loss: 1.7188 - categorical_accuracy: 0.2402\n",
            "Epoch 3/30\n",
            "190/190 [==============================] - 1s 8ms/step - loss: 1.6934 - categorical_accuracy: 0.2580\n",
            "Epoch 4/30\n",
            "190/190 [==============================] - 1s 8ms/step - loss: 1.6636 - categorical_accuracy: 0.2844\n",
            "Epoch 5/30\n",
            "190/190 [==============================] - 2s 11ms/step - loss: 1.6144 - categorical_accuracy: 0.3059\n",
            "Epoch 6/30\n",
            "190/190 [==============================] - 2s 12ms/step - loss: 1.5710 - categorical_accuracy: 0.3487\n",
            "Epoch 7/30\n",
            "190/190 [==============================] - 2s 12ms/step - loss: 1.5086 - categorical_accuracy: 0.3680\n",
            "Epoch 8/30\n",
            "190/190 [==============================] - 1s 7ms/step - loss: 1.4477 - categorical_accuracy: 0.4059\n",
            "Epoch 9/30\n",
            "190/190 [==============================] - 1s 7ms/step - loss: 1.3756 - categorical_accuracy: 0.4480\n",
            "Epoch 10/30\n",
            "190/190 [==============================] - 1s 7ms/step - loss: 1.3161 - categorical_accuracy: 0.4732\n",
            "Epoch 11/30\n",
            "190/190 [==============================] - 1s 7ms/step - loss: 1.2332 - categorical_accuracy: 0.5188\n",
            "Epoch 12/30\n",
            "190/190 [==============================] - 1s 8ms/step - loss: 1.1765 - categorical_accuracy: 0.5450\n",
            "Epoch 13/30\n",
            "190/190 [==============================] - 1s 8ms/step - loss: 1.1073 - categorical_accuracy: 0.5777\n",
            "Epoch 14/30\n",
            "190/190 [==============================] - 2s 8ms/step - loss: 1.0367 - categorical_accuracy: 0.5975\n",
            "Epoch 15/30\n",
            "190/190 [==============================] - 2s 12ms/step - loss: 0.9758 - categorical_accuracy: 0.6301\n",
            "Epoch 16/30\n",
            "190/190 [==============================] - 2s 13ms/step - loss: 0.8911 - categorical_accuracy: 0.6621\n",
            "Epoch 17/30\n",
            "190/190 [==============================] - 2s 10ms/step - loss: 0.8463 - categorical_accuracy: 0.6811\n",
            "Epoch 18/30\n",
            "190/190 [==============================] - 1s 7ms/step - loss: 0.8051 - categorical_accuracy: 0.7028\n",
            "Epoch 19/30\n",
            "190/190 [==============================] - 1s 8ms/step - loss: 0.7876 - categorical_accuracy: 0.7127\n",
            "Epoch 20/30\n",
            "190/190 [==============================] - 1s 8ms/step - loss: 0.7316 - categorical_accuracy: 0.7324\n",
            "Epoch 21/30\n",
            "190/190 [==============================] - 1s 8ms/step - loss: 0.6487 - categorical_accuracy: 0.7604\n",
            "Epoch 22/30\n",
            "190/190 [==============================] - 1s 8ms/step - loss: 0.6420 - categorical_accuracy: 0.7694\n",
            "Epoch 23/30\n",
            "190/190 [==============================] - 1s 8ms/step - loss: 0.5901 - categorical_accuracy: 0.7860\n",
            "Epoch 24/30\n",
            "190/190 [==============================] - 2s 11ms/step - loss: 0.6192 - categorical_accuracy: 0.7843\n",
            "Epoch 25/30\n",
            "190/190 [==============================] - 2s 13ms/step - loss: 0.5640 - categorical_accuracy: 0.8013\n",
            "Epoch 26/30\n",
            "190/190 [==============================] - 2s 12ms/step - loss: 0.5364 - categorical_accuracy: 0.8144\n",
            "Epoch 27/30\n",
            "190/190 [==============================] - 1s 8ms/step - loss: 0.5168 - categorical_accuracy: 0.8165\n",
            "Epoch 28/30\n",
            "190/190 [==============================] - 1s 8ms/step - loss: 0.5281 - categorical_accuracy: 0.8190\n",
            "Epoch 29/30\n",
            "190/190 [==============================] - 1s 8ms/step - loss: 0.4671 - categorical_accuracy: 0.8418\n",
            "Epoch 30/30\n",
            "190/190 [==============================] - 1s 8ms/step - loss: 0.4553 - categorical_accuracy: 0.8444\n",
            "22/22 [==============================] - 0s 6ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = keras.utils.to_categorical(predictions, num_classes=6)\n",
        "print(classification_report(y_test, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfeazgluxsAQ",
        "outputId": "c790ffc4-b8d3-4663-a50c-f208034530ee"
      },
      "execution_count": 452,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.19      0.12      0.14        43\n",
            "           1       0.25      0.25      0.25       167\n",
            "           2       0.22      0.17      0.19       139\n",
            "           3       0.21      0.23      0.22       182\n",
            "           4       0.24      0.36      0.29       173\n",
            "           5       0.20      0.13      0.16       149\n",
            "\n",
            "   micro avg       0.23      0.23      0.23       853\n",
            "   macro avg       0.22      0.21      0.21       853\n",
            "weighted avg       0.22      0.23      0.22       853\n",
            " samples avg       0.23      0.23      0.23       853\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for kernel in kernel_sizes:\n",
        "  x_1 = Conv1D(filters=filter_size,kernel_size=kernel)(x_stmt)\n",
        "  x_1 = GlobalMaxPool1D()(x_1)\n",
        "  filter_without_pos.append(x_1)\n",
        "    \n",
        "  x_2 = Conv1D(filters=filter_size,kernel_size=kernel)(x_pos)\n",
        "  x_2 = GlobalMaxPool1D()(x_2)\n",
        "  filter_with_pos.append(x_2)\n",
        "\n",
        "conv_in1 = keras.layers.concatenate(filter_without_pos)\n",
        "conv_in1 = Dropout(0.2)(conv_in1)\n",
        "conv_in1 = Dense(128, activation='relu')(conv_in1)\n",
        "conv_in2 = keras.layers.concatenate(filter_with_pos)\n",
        "conv_in2 = Dropout(0.2)(conv_in2)\n",
        "conv_in2 = Dense(128, activation='relu')(conv_in2)\n",
        "\n",
        "x = conv_in1\n",
        "if use_pos:\n",
        "  x = keras.layers.concatenate([conv_in1, conv_in2])\n",
        "else:\n",
        "  x = conv_in1\n",
        "\n",
        "output = Dense(6, activation='softmax', name='main_output')(x)\n",
        "\n",
        "if use_pos:\n",
        "  model_cnn = Model(inputs=[statement_input, pos_input], outputs=[output])\n",
        "else:\n",
        "  model_cnn = Model(inputs=[statement_input], outputs=[output])"
      ],
      "metadata": {
        "id": "xt1uoXs6GM-9"
      },
      "execution_count": 473,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = train(model_cnn,'cnn',use_pos=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsvkPxcd0VDp",
        "outputId": "c283e148-7315-4a23-f773-9c77f4c2a733"
      },
      "execution_count": 474,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "190/190 [==============================] - 3s 13ms/step - loss: 1.7999 - categorical_accuracy: 0.2090\n",
            "Epoch 2/30\n",
            "190/190 [==============================] - 2s 13ms/step - loss: 1.7198 - categorical_accuracy: 0.2388\n",
            "Epoch 3/30\n",
            "190/190 [==============================] - 2s 13ms/step - loss: 1.7010 - categorical_accuracy: 0.2556\n",
            "Epoch 4/30\n",
            "190/190 [==============================] - 3s 16ms/step - loss: 1.6701 - categorical_accuracy: 0.2786\n",
            "Epoch 5/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 1.6308 - categorical_accuracy: 0.3030\n",
            "Epoch 6/30\n",
            "190/190 [==============================] - 2s 13ms/step - loss: 1.5722 - categorical_accuracy: 0.3388\n",
            "Epoch 7/30\n",
            "190/190 [==============================] - 2s 13ms/step - loss: 1.5410 - categorical_accuracy: 0.3653\n",
            "Epoch 8/30\n",
            "190/190 [==============================] - 2s 13ms/step - loss: 1.4557 - categorical_accuracy: 0.4128\n",
            "Epoch 9/30\n",
            "190/190 [==============================] - 2s 13ms/step - loss: 1.3591 - categorical_accuracy: 0.4674\n",
            "Epoch 10/30\n",
            "190/190 [==============================] - 4s 21ms/step - loss: 1.2801 - categorical_accuracy: 0.4960\n",
            "Epoch 11/30\n",
            "190/190 [==============================] - 3s 18ms/step - loss: 1.1986 - categorical_accuracy: 0.5365\n",
            "Epoch 12/30\n",
            "190/190 [==============================] - 2s 13ms/step - loss: 1.1640 - categorical_accuracy: 0.5662\n",
            "Epoch 13/30\n",
            "190/190 [==============================] - 2s 13ms/step - loss: 1.0064 - categorical_accuracy: 0.6207\n",
            "Epoch 14/30\n",
            "190/190 [==============================] - 2s 13ms/step - loss: 0.9365 - categorical_accuracy: 0.6605\n",
            "Epoch 15/30\n",
            "190/190 [==============================] - 3s 15ms/step - loss: 0.8388 - categorical_accuracy: 0.6914\n",
            "Epoch 16/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 0.7298 - categorical_accuracy: 0.7349\n",
            "Epoch 17/30\n",
            "190/190 [==============================] - 3s 14ms/step - loss: 0.6401 - categorical_accuracy: 0.7726\n",
            "Epoch 18/30\n",
            "190/190 [==============================] - 2s 13ms/step - loss: 0.6397 - categorical_accuracy: 0.7719\n",
            "Epoch 19/30\n",
            "190/190 [==============================] - 2s 13ms/step - loss: 0.6012 - categorical_accuracy: 0.7921\n",
            "Epoch 20/30\n",
            "190/190 [==============================] - 2s 13ms/step - loss: 0.5397 - categorical_accuracy: 0.8150\n",
            "Epoch 21/30\n",
            "190/190 [==============================] - 4s 20ms/step - loss: 0.4763 - categorical_accuracy: 0.8381\n",
            "Epoch 22/30\n",
            "190/190 [==============================] - 4s 19ms/step - loss: 0.4620 - categorical_accuracy: 0.8421\n",
            "Epoch 23/30\n",
            "190/190 [==============================] - 2s 13ms/step - loss: 0.4674 - categorical_accuracy: 0.8446\n",
            "Epoch 24/30\n",
            "190/190 [==============================] - 2s 13ms/step - loss: 0.4129 - categorical_accuracy: 0.8666\n",
            "Epoch 25/30\n",
            "190/190 [==============================] - 2s 13ms/step - loss: 0.3115 - categorical_accuracy: 0.8941\n",
            "Epoch 26/30\n",
            "190/190 [==============================] - 3s 16ms/step - loss: 0.3747 - categorical_accuracy: 0.8726\n",
            "Epoch 27/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 0.3228 - categorical_accuracy: 0.8945\n",
            "Epoch 28/30\n",
            "190/190 [==============================] - 3s 14ms/step - loss: 0.3134 - categorical_accuracy: 0.8985\n",
            "Epoch 29/30\n",
            "190/190 [==============================] - 2s 13ms/step - loss: 0.2269 - categorical_accuracy: 0.9258\n",
            "Epoch 30/30\n",
            "190/190 [==============================] - 2s 13ms/step - loss: 0.2397 - categorical_accuracy: 0.9213\n",
            "22/22 [==============================] - 0s 6ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = keras.utils.to_categorical(predictions, num_classes=6)\n",
        "print(classification_report(y_test, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flX0OSNd0Xf6",
        "outputId": "1d329472-c656-4828-9ef2-3279460dcb00"
      },
      "execution_count": 475,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        43\n",
            "           1       0.24      0.20      0.22       167\n",
            "           2       0.22      0.13      0.16       139\n",
            "           3       0.23      0.16      0.19       182\n",
            "           4       0.23      0.60      0.34       173\n",
            "           5       0.25      0.06      0.10       149\n",
            "\n",
            "   micro avg       0.23      0.23      0.23       853\n",
            "   macro avg       0.20      0.19      0.17       853\n",
            "weighted avg       0.22      0.23      0.20       853\n",
            " samples avg       0.23      0.23      0.23       853\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM Model**"
      ],
      "metadata": {
        "id": "RW07YGjJE-re"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "statement_input = Input(shape=(num_steps,), dtype='int32', name='main_input')\n",
        "x = Embedding(vocab_length+1,emb_dimension,weights=[emb_matrix],input_length=num_steps,trainable=False)(statement_input) \n",
        "lstm_in = LSTM(lstm_size,dropout=0.2)(x)\n",
        "pos_input = Input(shape=(num_steps,), dtype='int32', name='pos_input')\n",
        "x2 = Embedding(max(pos_dict.values()), max(pos_dict.values()), weights=[pos_embeddings], input_length=num_steps, trainable=False)(pos_input)\n",
        "lstm_in2 = LSTM(lstm_size, dropout=0.2)(x2)"
      ],
      "metadata": {
        "id": "qnX4FQpe3pDc"
      },
      "execution_count": 456,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM model\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(vocab_length+1, hidden_size, input_length=num_steps))\n",
        "model_lstm.add(Dense(6, activation='softmax'))\n",
        "\n",
        "\n",
        "\n",
        "if use_pos :\n",
        "  x = keras.layers.concatenate([lstm_in, lstm_in2])\n",
        "else:\n",
        "  x = lstm_in\n",
        "\n",
        "main_output = Dense(6, activation='softmax', name='main_output')(x)\n",
        "\n",
        "if use_pos:\n",
        "  model_lstm = Model(inputs=[statement_input, pos_input], outputs=[main_output])\n",
        "else:\n",
        "  model_lstm = Model(inputs=[statement_input], outputs=[main_output])"
      ],
      "metadata": {
        "id": "HTbdzzEC5qr6"
      },
      "execution_count": 457,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = train(model_lstm,'lstm',use_pos=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4nx4ldv5u1u",
        "outputId": "e3d5f3af-14ae-4e1a-808d-30e50a8a76a9"
      },
      "execution_count": 458,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "190/190 [==============================] - 9s 37ms/step - loss: 1.7419 - categorical_accuracy: 0.2252\n",
            "Epoch 2/30\n",
            "190/190 [==============================] - 4s 21ms/step - loss: 1.7202 - categorical_accuracy: 0.2369\n",
            "Epoch 3/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 1.7086 - categorical_accuracy: 0.2473\n",
            "Epoch 4/30\n",
            "190/190 [==============================] - 7s 35ms/step - loss: 1.7021 - categorical_accuracy: 0.2566\n",
            "Epoch 5/30\n",
            "190/190 [==============================] - 4s 23ms/step - loss: 1.6962 - categorical_accuracy: 0.2626\n",
            "Epoch 6/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 1.6895 - categorical_accuracy: 0.2719\n",
            "Epoch 7/30\n",
            "190/190 [==============================] - 6s 32ms/step - loss: 1.6844 - categorical_accuracy: 0.2732\n",
            "Epoch 8/30\n",
            "190/190 [==============================] - 5s 27ms/step - loss: 1.6744 - categorical_accuracy: 0.2862\n",
            "Epoch 9/30\n",
            "190/190 [==============================] - 4s 21ms/step - loss: 1.6748 - categorical_accuracy: 0.2780\n",
            "Epoch 10/30\n",
            "190/190 [==============================] - 5s 27ms/step - loss: 1.6678 - categorical_accuracy: 0.2823\n",
            "Epoch 11/30\n",
            "190/190 [==============================] - 6s 31ms/step - loss: 1.6601 - categorical_accuracy: 0.2893\n",
            "Epoch 12/30\n",
            "190/190 [==============================] - 4s 21ms/step - loss: 1.6575 - categorical_accuracy: 0.2931\n",
            "Epoch 13/30\n",
            "190/190 [==============================] - 4s 23ms/step - loss: 1.6478 - categorical_accuracy: 0.2956\n",
            "Epoch 14/30\n",
            "190/190 [==============================] - 7s 35ms/step - loss: 1.6394 - categorical_accuracy: 0.2999\n",
            "Epoch 15/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 1.6399 - categorical_accuracy: 0.3023\n",
            "Epoch 16/30\n",
            "190/190 [==============================] - 4s 21ms/step - loss: 1.6275 - categorical_accuracy: 0.3085\n",
            "Epoch 17/30\n",
            "190/190 [==============================] - 7s 37ms/step - loss: 1.6170 - categorical_accuracy: 0.3107\n",
            "Epoch 18/30\n",
            "190/190 [==============================] - 4s 21ms/step - loss: 1.6124 - categorical_accuracy: 0.3202\n",
            "Epoch 19/30\n",
            "190/190 [==============================] - 4s 21ms/step - loss: 1.6086 - categorical_accuracy: 0.3258\n",
            "Epoch 20/30\n",
            "190/190 [==============================] - 7s 37ms/step - loss: 1.5905 - categorical_accuracy: 0.3333\n",
            "Epoch 21/30\n",
            "190/190 [==============================] - 4s 21ms/step - loss: 1.5773 - categorical_accuracy: 0.3448\n",
            "Epoch 22/30\n",
            "190/190 [==============================] - 4s 21ms/step - loss: 1.5729 - categorical_accuracy: 0.3475\n",
            "Epoch 23/30\n",
            "190/190 [==============================] - 6s 34ms/step - loss: 1.5614 - categorical_accuracy: 0.3511\n",
            "Epoch 24/30\n",
            "190/190 [==============================] - 5s 24ms/step - loss: 1.5471 - categorical_accuracy: 0.3507\n",
            "Epoch 25/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 1.5384 - categorical_accuracy: 0.3610\n",
            "Epoch 26/30\n",
            "190/190 [==============================] - 6s 31ms/step - loss: 1.5221 - categorical_accuracy: 0.3728\n",
            "Epoch 27/30\n",
            "190/190 [==============================] - 5s 27ms/step - loss: 1.5046 - categorical_accuracy: 0.3822\n",
            "Epoch 28/30\n",
            "190/190 [==============================] - 4s 21ms/step - loss: 1.4932 - categorical_accuracy: 0.3872\n",
            "Epoch 29/30\n",
            "190/190 [==============================] - 5s 26ms/step - loss: 1.4705 - categorical_accuracy: 0.4017\n",
            "Epoch 30/30\n",
            "190/190 [==============================] - 6s 32ms/step - loss: 1.4529 - categorical_accuracy: 0.4173\n",
            "22/22 [==============================] - 1s 16ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = keras.utils.to_categorical(predictions, num_classes=6)\n",
        "print(classification_report(y_test, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cI7Ru6Up5ydu",
        "outputId": "842b3419-b4fe-48a3-d81b-b5be3eae37cd"
      },
      "execution_count": 459,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.08      0.19      0.11        43\n",
            "           1       0.29      0.23      0.26       167\n",
            "           2       0.27      0.12      0.17       139\n",
            "           3       0.23      0.29      0.26       182\n",
            "           4       0.20      0.17      0.18       173\n",
            "           5       0.23      0.28      0.25       149\n",
            "\n",
            "   micro avg       0.22      0.22      0.22       853\n",
            "   macro avg       0.22      0.21      0.20       853\n",
            "weighted avg       0.23      0.22      0.22       853\n",
            " samples avg       0.22      0.22      0.22       853\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM model\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(vocab_length+1, hidden_size, input_length=num_steps))\n",
        "model_lstm.add(Dense(6, activation='softmax'))\n",
        "\n",
        "\n",
        "\n",
        "if use_pos :\n",
        "  x = keras.layers.concatenate([lstm_in, lstm_in2])\n",
        "else:\n",
        "  x = lstm_in\n",
        "\n",
        "main_output = Dense(6, activation='softmax', name='main_output')(x)\n",
        "\n",
        "if use_pos:\n",
        "  model_lstm = Model(inputs=[statement_input, pos_input], outputs=[main_output])\n",
        "else:\n",
        "  model_lstm = Model(inputs=[statement_input], outputs=[main_output])"
      ],
      "metadata": {
        "id": "xGaA_Jr1JK9Q"
      },
      "execution_count": 460,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = train(model_lstm,'lstm',use_pos=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6aOp-y46CsH",
        "outputId": "67ec4380-174f-4abc-e2b0-6c3eae78d49f"
      },
      "execution_count": 461,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "190/190 [==============================] - 6s 21ms/step - loss: 1.5714 - categorical_accuracy: 0.3421\n",
            "Epoch 2/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 1.4701 - categorical_accuracy: 0.4109\n",
            "Epoch 3/30\n",
            "190/190 [==============================] - 7s 37ms/step - loss: 1.4369 - categorical_accuracy: 0.4182\n",
            "Epoch 4/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 1.4052 - categorical_accuracy: 0.4394\n",
            "Epoch 5/30\n",
            "190/190 [==============================] - 4s 21ms/step - loss: 1.3865 - categorical_accuracy: 0.4434\n",
            "Epoch 6/30\n",
            "190/190 [==============================] - 7s 37ms/step - loss: 1.3553 - categorical_accuracy: 0.4662\n",
            "Epoch 7/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 1.3301 - categorical_accuracy: 0.4676\n",
            "Epoch 8/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 1.3037 - categorical_accuracy: 0.4953\n",
            "Epoch 9/30\n",
            "190/190 [==============================] - 7s 36ms/step - loss: 1.2745 - categorical_accuracy: 0.5028\n",
            "Epoch 10/30\n",
            "190/190 [==============================] - 4s 23ms/step - loss: 1.2651 - categorical_accuracy: 0.5061\n",
            "Epoch 11/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 1.2308 - categorical_accuracy: 0.5218\n",
            "Epoch 12/30\n",
            "190/190 [==============================] - 6s 34ms/step - loss: 1.1975 - categorical_accuracy: 0.5342\n",
            "Epoch 13/30\n",
            "190/190 [==============================] - 5s 25ms/step - loss: 1.1768 - categorical_accuracy: 0.5441\n",
            "Epoch 14/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 1.1520 - categorical_accuracy: 0.5574\n",
            "Epoch 15/30\n",
            "190/190 [==============================] - 6s 31ms/step - loss: 1.1205 - categorical_accuracy: 0.5682\n",
            "Epoch 16/30\n",
            "190/190 [==============================] - 5s 28ms/step - loss: 1.1048 - categorical_accuracy: 0.5751\n",
            "Epoch 17/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 1.0683 - categorical_accuracy: 0.5921\n",
            "Epoch 18/30\n",
            "190/190 [==============================] - 5s 28ms/step - loss: 1.0482 - categorical_accuracy: 0.6037\n",
            "Epoch 19/30\n",
            "190/190 [==============================] - 7s 36ms/step - loss: 1.0248 - categorical_accuracy: 0.6096\n",
            "Epoch 20/30\n",
            "190/190 [==============================] - 6s 34ms/step - loss: 1.0039 - categorical_accuracy: 0.6163\n",
            "Epoch 21/30\n",
            "190/190 [==============================] - 7s 38ms/step - loss: 0.9593 - categorical_accuracy: 0.6339\n",
            "Epoch 22/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 0.9463 - categorical_accuracy: 0.6494\n",
            "Epoch 23/30\n",
            "190/190 [==============================] - 4s 21ms/step - loss: 0.9266 - categorical_accuracy: 0.6497\n",
            "Epoch 24/30\n",
            "190/190 [==============================] - 7s 37ms/step - loss: 0.8972 - categorical_accuracy: 0.6699\n",
            "Epoch 25/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 0.8841 - categorical_accuracy: 0.6678\n",
            "Epoch 26/30\n",
            "190/190 [==============================] - 4s 21ms/step - loss: 0.8597 - categorical_accuracy: 0.6792\n",
            "Epoch 27/30\n",
            "190/190 [==============================] - 7s 36ms/step - loss: 0.8275 - categorical_accuracy: 0.6929\n",
            "Epoch 28/30\n",
            "190/190 [==============================] - 4s 23ms/step - loss: 0.8186 - categorical_accuracy: 0.6998\n",
            "Epoch 29/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 0.8050 - categorical_accuracy: 0.7020\n",
            "Epoch 30/30\n",
            "190/190 [==============================] - 6s 34ms/step - loss: 0.7845 - categorical_accuracy: 0.7072\n",
            "22/22 [==============================] - 1s 9ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = keras.utils.to_categorical(predictions, num_classes=6)\n",
        "print(classification_report(y_test, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reqcCokx6GBA",
        "outputId": "144f9775-d6b0-4095-f43b-116c22dcca19"
      },
      "execution_count": 462,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.15      0.12      0.13        43\n",
            "           1       0.23      0.27      0.25       167\n",
            "           2       0.19      0.21      0.20       139\n",
            "           3       0.24      0.18      0.20       182\n",
            "           4       0.26      0.27      0.26       173\n",
            "           5       0.20      0.21      0.21       149\n",
            "\n",
            "   micro avg       0.22      0.22      0.22       853\n",
            "   macro avg       0.21      0.21      0.21       853\n",
            "weighted avg       0.22      0.22      0.22       853\n",
            " samples avg       0.22      0.22      0.22       853\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "statement_input = Input(shape=(num_steps,), dtype='int32', name='main_input')\n",
        "x = Embedding(vocab_length+1,emb_dimension,weights=[emb_matrix],input_length=num_steps,trainable=False)(statement_input) \n",
        "lstm_in = LSTM(lstm_size,dropout=0.2)(x)\n",
        "pos_input = Input(shape=(num_steps,), dtype='int32', name='pos_input')\n",
        "x2 = Embedding(max(pos_dict.values()), max(pos_dict.values()), weights=[pos_embeddings], input_length=num_steps, trainable=False)(pos_input)\n",
        "lstm_in2 = LSTM(lstm_size, dropout=0.2)(x2)"
      ],
      "metadata": {
        "id": "ZJcE7LgS7Jhg"
      },
      "execution_count": 463,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BiLSTM model\n",
        "model_bilstm = Sequential()\n",
        "model_bilstm.add(Embedding(vocab_length+1, hidden_size, input_length=num_steps))\n",
        "model_bilstm.add(Bidirectional(LSTM(hidden_size)))\n",
        "model_bilstm.add(Dense(6, activation='softmax'))\n",
        "\n",
        "\n",
        "\n",
        "if use_pos :\n",
        "  x = keras.layers.concatenate([lstm_in, lstm_in2])\n",
        "else:\n",
        "  x = lstm_in\n",
        "\n",
        "main_output = Dense(6, activation='softmax', name='main_output')(x)\n",
        "\n",
        "if use_pos:\n",
        "  model_bilstm = Model(inputs=[statement_input, pos_input], outputs=[main_output])\n",
        "else:\n",
        "  model_bilstm = Model(inputs=[statement_input], outputs=[main_output])"
      ],
      "metadata": {
        "id": "zq7lwYMy3hoO"
      },
      "execution_count": 464,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = train(model_bilstm,'BiLSTM',use_pos=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tokFW2Zd4oTo",
        "outputId": "8f21e88a-7f79-4208-848b-0bc6a22f38a7"
      },
      "execution_count": 465,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "190/190 [==============================] - 9s 36ms/step - loss: 1.7424 - categorical_accuracy: 0.2208\n",
            "Epoch 2/30\n",
            "190/190 [==============================] - 4s 23ms/step - loss: 1.7168 - categorical_accuracy: 0.2428\n",
            "Epoch 3/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 1.7087 - categorical_accuracy: 0.2481\n",
            "Epoch 4/30\n",
            "190/190 [==============================] - 6s 33ms/step - loss: 1.7028 - categorical_accuracy: 0.2566\n",
            "Epoch 5/30\n",
            "190/190 [==============================] - 5s 26ms/step - loss: 1.6968 - categorical_accuracy: 0.2632\n",
            "Epoch 6/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 1.6892 - categorical_accuracy: 0.2692\n",
            "Epoch 7/30\n",
            "190/190 [==============================] - 6s 31ms/step - loss: 1.6859 - categorical_accuracy: 0.2712\n",
            "Epoch 8/30\n",
            "190/190 [==============================] - 5s 28ms/step - loss: 1.6817 - categorical_accuracy: 0.2709\n",
            "Epoch 9/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 1.6760 - categorical_accuracy: 0.2796\n",
            "Epoch 10/30\n",
            "190/190 [==============================] - 5s 28ms/step - loss: 1.6685 - categorical_accuracy: 0.2803\n",
            "Epoch 11/30\n",
            "190/190 [==============================] - 6s 30ms/step - loss: 1.6649 - categorical_accuracy: 0.2850\n",
            "Epoch 12/30\n",
            "190/190 [==============================] - 4s 21ms/step - loss: 1.6604 - categorical_accuracy: 0.2974\n",
            "Epoch 13/30\n",
            "190/190 [==============================] - 5s 25ms/step - loss: 1.6551 - categorical_accuracy: 0.2889\n",
            "Epoch 14/30\n",
            "190/190 [==============================] - 6s 33ms/step - loss: 1.6482 - categorical_accuracy: 0.2984\n",
            "Epoch 15/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 1.6452 - categorical_accuracy: 0.2937\n",
            "Epoch 16/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 1.6367 - categorical_accuracy: 0.3097\n",
            "Epoch 17/30\n",
            "190/190 [==============================] - 7s 37ms/step - loss: 1.6268 - categorical_accuracy: 0.3142\n",
            "Epoch 18/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 1.6242 - categorical_accuracy: 0.3160\n",
            "Epoch 19/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 1.6107 - categorical_accuracy: 0.3258\n",
            "Epoch 20/30\n",
            "190/190 [==============================] - 7s 38ms/step - loss: 1.5989 - categorical_accuracy: 0.3316\n",
            "Epoch 21/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 1.5958 - categorical_accuracy: 0.3328\n",
            "Epoch 22/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 1.5836 - categorical_accuracy: 0.3366\n",
            "Epoch 23/30\n",
            "190/190 [==============================] - 7s 37ms/step - loss: 1.5717 - categorical_accuracy: 0.3525\n",
            "Epoch 24/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 1.5557 - categorical_accuracy: 0.3583\n",
            "Epoch 25/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 1.5506 - categorical_accuracy: 0.3623\n",
            "Epoch 26/30\n",
            "190/190 [==============================] - 7s 37ms/step - loss: 1.5317 - categorical_accuracy: 0.3699\n",
            "Epoch 27/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 1.5115 - categorical_accuracy: 0.3792\n",
            "Epoch 28/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 1.4962 - categorical_accuracy: 0.3935\n",
            "Epoch 29/30\n",
            "190/190 [==============================] - 7s 36ms/step - loss: 1.4838 - categorical_accuracy: 0.4013\n",
            "Epoch 30/30\n",
            "190/190 [==============================] - 5s 24ms/step - loss: 1.4692 - categorical_accuracy: 0.4071\n",
            "22/22 [==============================] - 1s 10ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = keras.utils.to_categorical(predictions, num_classes=6)\n",
        "print(classification_report(y_test, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igT7xwig4vxi",
        "outputId": "4186c59f-0f7b-435a-bff8-25906693c992"
      },
      "execution_count": 466,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.22      0.05      0.08        43\n",
            "           1       0.26      0.35      0.30       167\n",
            "           2       0.26      0.10      0.15       139\n",
            "           3       0.26      0.24      0.25       182\n",
            "           4       0.24      0.39      0.30       173\n",
            "           5       0.24      0.19      0.22       149\n",
            "\n",
            "   micro avg       0.25      0.25      0.25       853\n",
            "   macro avg       0.25      0.22      0.21       853\n",
            "weighted avg       0.25      0.25      0.24       853\n",
            " samples avg       0.25      0.25      0.25       853\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BiLSTM model\n",
        "model_bilstm = Sequential()\n",
        "model_bilstm.add(Embedding(vocab_length+1, hidden_size, input_length=num_steps))\n",
        "model_bilstm.add(Bidirectional(LSTM(hidden_size)))\n",
        "model_bilstm.add(Dense(6, activation='softmax'))\n",
        "\n",
        "\n",
        "\n",
        "if use_pos :\n",
        "  x = keras.layers.concatenate([lstm_in, lstm_in2])\n",
        "else:\n",
        "  x = lstm_in\n",
        "\n",
        "main_output = Dense(6, activation='softmax', name='main_output')(x)\n",
        "\n",
        "if use_pos:\n",
        "  model_bilstm = Model(inputs=[statement_input, pos_input], outputs=[main_output])\n",
        "else:\n",
        "  model_bilstm = Model(inputs=[statement_input], outputs=[main_output])"
      ],
      "metadata": {
        "id": "OLmYqcPOJvQW"
      },
      "execution_count": 467,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = train(model_bilstm,'BiLSTM',use_pos=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZW2JJ5mnJxKl",
        "outputId": "6c96009d-c414-4dca-8de4-0e1086fb5ad2"
      },
      "execution_count": 468,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "190/190 [==============================] - 10s 35ms/step - loss: 1.5729 - categorical_accuracy: 0.3520\n",
            "Epoch 2/30\n",
            "190/190 [==============================] - 5s 24ms/step - loss: 1.4894 - categorical_accuracy: 0.3931\n",
            "Epoch 3/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 1.4596 - categorical_accuracy: 0.4150\n",
            "Epoch 4/30\n",
            "190/190 [==============================] - 6s 33ms/step - loss: 1.4221 - categorical_accuracy: 0.4327\n",
            "Epoch 5/30\n",
            "190/190 [==============================] - 5s 27ms/step - loss: 1.3939 - categorical_accuracy: 0.4510\n",
            "Epoch 6/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 1.3691 - categorical_accuracy: 0.4571\n",
            "Epoch 7/30\n",
            "190/190 [==============================] - 6s 31ms/step - loss: 1.3462 - categorical_accuracy: 0.4704\n",
            "Epoch 8/30\n",
            "190/190 [==============================] - 5s 29ms/step - loss: 1.3186 - categorical_accuracy: 0.4761\n",
            "Epoch 9/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 1.2964 - categorical_accuracy: 0.4928\n",
            "Epoch 10/30\n",
            "190/190 [==============================] - 6s 29ms/step - loss: 1.2604 - categorical_accuracy: 0.5063\n",
            "Epoch 11/30\n",
            "190/190 [==============================] - 6s 30ms/step - loss: 1.2333 - categorical_accuracy: 0.5204\n",
            "Epoch 12/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 1.2170 - categorical_accuracy: 0.5283\n",
            "Epoch 13/30\n",
            "190/190 [==============================] - 5s 27ms/step - loss: 1.1867 - categorical_accuracy: 0.5391\n",
            "Epoch 14/30\n",
            "190/190 [==============================] - 6s 32ms/step - loss: 1.1595 - categorical_accuracy: 0.5490\n",
            "Epoch 15/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 1.1372 - categorical_accuracy: 0.5603\n",
            "Epoch 16/30\n",
            "190/190 [==============================] - 5s 26ms/step - loss: 1.1009 - categorical_accuracy: 0.5779\n",
            "Epoch 17/30\n",
            "190/190 [==============================] - 6s 33ms/step - loss: 1.0741 - categorical_accuracy: 0.5804\n",
            "Epoch 18/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 1.0590 - categorical_accuracy: 0.5930\n",
            "Epoch 19/30\n",
            "190/190 [==============================] - 4s 23ms/step - loss: 1.0193 - categorical_accuracy: 0.6113\n",
            "Epoch 20/30\n",
            "190/190 [==============================] - 7s 36ms/step - loss: 0.9995 - categorical_accuracy: 0.6202\n",
            "Epoch 21/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 0.9568 - categorical_accuracy: 0.6422\n",
            "Epoch 22/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 0.9564 - categorical_accuracy: 0.6410\n",
            "Epoch 23/30\n",
            "190/190 [==============================] - 7s 37ms/step - loss: 0.9086 - categorical_accuracy: 0.6580\n",
            "Epoch 24/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 0.8964 - categorical_accuracy: 0.6642\n",
            "Epoch 25/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 0.8734 - categorical_accuracy: 0.6770\n",
            "Epoch 26/30\n",
            "190/190 [==============================] - 7s 37ms/step - loss: 0.8508 - categorical_accuracy: 0.6807\n",
            "Epoch 27/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 0.8326 - categorical_accuracy: 0.6869\n",
            "Epoch 28/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 0.8146 - categorical_accuracy: 0.6972\n",
            "Epoch 29/30\n",
            "190/190 [==============================] - 7s 37ms/step - loss: 0.7937 - categorical_accuracy: 0.7038\n",
            "Epoch 30/30\n",
            "190/190 [==============================] - 4s 22ms/step - loss: 0.7769 - categorical_accuracy: 0.7093\n",
            "22/22 [==============================] - 1s 9ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = keras.utils.to_categorical(predictions, num_classes=6)\n",
        "print(classification_report(y_test, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1DOxO8w45mW",
        "outputId": "9e35ee61-4073-4c0f-f871-fb61f566a017"
      },
      "execution_count": 469,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.14      0.14      0.14        43\n",
            "           1       0.26      0.25      0.25       167\n",
            "           2       0.23      0.18      0.20       139\n",
            "           3       0.22      0.26      0.24       182\n",
            "           4       0.18      0.26      0.22       173\n",
            "           5       0.26      0.15      0.19       149\n",
            "\n",
            "   micro avg       0.22      0.22      0.22       853\n",
            "   macro avg       0.22      0.21      0.21       853\n",
            "weighted avg       0.22      0.22      0.22       853\n",
            " samples avg       0.22      0.22      0.22       853\n",
            "\n"
          ]
        }
      ]
    }
  ]
}