{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mz9lM0Dq2Ie5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c66354b-8f2b-4653-dee0-e1ddaa6fb172"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (0.15.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.98-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (16.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers) (8.1.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->sentence-transformers) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.12)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125942 sha256=8c172d3117f7105928891bfd33483735cbef8d949f31b99d34bf57eecc499dd0\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/67/06/162a3760c40d74dd40bc855d527008d26341c2b0ecf3e8e11f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, huggingface-hub, transformers, sentence-transformers\n",
            "Successfully installed huggingface-hub-0.13.4 sentence-transformers-2.2.2 sentencepiece-0.1.98 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Wno6LzAa2MLB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b632cc6a-2d4f-4b2f-d955-0e7e190c0028"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Conv1D, GlobalMaxPooling1D\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Dropout, Conv1D, GlobalMaxPooling1D\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import re\n",
        "import glob\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "import sklearn\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
        "from sklearn.preprocessing import Binarizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, losses, models, util\n",
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "from sentence_transformers.readers import InputExample\n",
        "\n",
        "import warnings\n",
        "from wordcloud import STOPWORDS, WordCloud\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "r1l4oDVf2RCq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8312e13f-0efc-4df1-c5f6-dd12f73b52b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FPkgFHW2U5K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91ea4328-de41-4052-89c9-4a16139ae67f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Datasets Sem-6/IR Datasets/Project/liar_dataset.zip\n",
            "  inflating: README                  \n",
            "  inflating: test.tsv                \n",
            "  inflating: train.tsv               \n",
            "  inflating: valid.tsv               \n"
          ]
        }
      ],
      "source": [
        "!unzip '/content/drive/MyDrive/Datasets Sem-6/IR Datasets/Project/liar_dataset.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hqd8FKSC2WQY"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('/content/drive/MyDrive/liar_dataset/train.tsv',sep='\\t', header = None)\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/liar_dataset/test.tsv',sep='\\t', header = None)\n",
        "val_df = pd.read_csv('/content/drive/MyDrive/liar_dataset/valid.tsv',sep='\\t', header = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qIWWW5eX2ZQO"
      },
      "outputs": [],
      "source": [
        "train_df = train_df.drop([0, 8, 9, 10, 11, 12], axis = 1)\n",
        "test_df = test_df.drop([0, 8, 9, 10, 11, 12], axis = 1)\n",
        "val_df = val_df.drop([0, 8, 9, 10, 11, 12], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "fa7fiiEp2bf2"
      },
      "outputs": [],
      "source": [
        "train_df.columns = ['label', 'statement', 'subject', 'speaker', 'speaker job title', 'state info', 'party affilation', 'location of statement']\n",
        "test_df.columns = ['label', 'statement', 'subject', 'speaker', 'speaker job title', 'state info', 'party affilation', 'location of statement']\n",
        "val_df.columns = ['label', 'statement', 'subject', 'speaker', 'speaker job title', 'state info', 'party affilation', 'location of statement']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8qtTzGjs3S5O"
      },
      "outputs": [],
      "source": [
        "train_df = train_df.dropna()\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "test_df = test_df.dropna()\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "val_df = val_df.dropna()\n",
        "val_df = val_df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BzJNV0mZ3UiC"
      },
      "outputs": [],
      "source": [
        "train_df = pd.concat([train_df, val_df])\n",
        "train_df = train_df.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "q4Vux97b3WWo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ca3665c-7004-4e7e-a52b-290a6544239e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data Dimensions - (7585, 9)\n",
            "Testing Data Dimensions - (853, 8)\n"
          ]
        }
      ],
      "source": [
        "print('Training Data Dimensions -', train_df.shape)\n",
        "print('Testing Data Dimensions -', test_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "eKSVBnVy3X55"
      },
      "outputs": [],
      "source": [
        "labels_dict = {'mostly-true':4,'barely-true':2,'half-true':3,'false':1, 'true':5,'pants-fire':0}\n",
        "train_df['label'] = train_df['label'].apply(lambda x: labels_dict[x])\n",
        "test_df['label'] = test_df['label'].apply(lambda x: labels_dict[x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "az8tc1nLBs-V"
      },
      "outputs": [],
      "source": [
        "def preprocess(text):\n",
        "  text = text.lower() # lower - casing the text\n",
        "  text = re.sub('<[^>]*>', ' ', text)\n",
        "  text = re.sub('[\\W]+', ' ', text)\n",
        "  tokenizer = TreebankWordTokenizer()\n",
        "  words = tokenizer.tokenize(text)\n",
        "  text = ' '.join(words)\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  word_tokens = word_tokenize(text)\n",
        "  filtered_sentence = [w for w in word_tokens if not w in stop_words] # removal of stopwords\n",
        "  text = ' '.join(filtered_sentence)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "tsJZNSKcBtQx"
      },
      "outputs": [],
      "source": [
        "# Standard Model['label' --> Target , 'statement' --> input text data]\n",
        "def get_XY(data):\n",
        "  y = data['label'].values\n",
        "  X = data['statement'].values\n",
        "  for i in range(len(X)):\n",
        "    X[i] = preprocess(X[i])\n",
        "  return X,y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNe2ry0NBnMK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05990c6d-a7af-4725-9c52-99c01fddab56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "190/190 [==============================] - 15s 71ms/step - loss: -2072.8821 - accuracy: 0.1938 - val_loss: -10735.0938 - val_accuracy: 0.1984\n",
            "Epoch 2/10\n",
            "190/190 [==============================] - 16s 83ms/step - loss: -80415.0312 - accuracy: 0.1943 - val_loss: -208881.4062 - val_accuracy: 0.1984\n",
            "Epoch 3/10\n",
            "190/190 [==============================] - 9s 46ms/step - loss: -606200.1250 - accuracy: 0.1943 - val_loss: -1132892.5000 - val_accuracy: 0.1984\n",
            "Epoch 4/10\n",
            "190/190 [==============================] - 11s 60ms/step - loss: -2323729.0000 - accuracy: 0.1943 - val_loss: -3617297.5000 - val_accuracy: 0.1984\n",
            "Epoch 5/10\n",
            "190/190 [==============================] - 11s 61ms/step - loss: -6202850.0000 - accuracy: 0.1943 - val_loss: -8660338.0000 - val_accuracy: 0.1984\n",
            "Epoch 6/10\n",
            "190/190 [==============================] - 9s 45ms/step - loss: -13321475.0000 - accuracy: 0.1943 - val_loss: -17326170.0000 - val_accuracy: 0.1984\n",
            "Epoch 7/10\n",
            "190/190 [==============================] - 11s 60ms/step - loss: -24884830.0000 - accuracy: 0.1943 - val_loss: -30784124.0000 - val_accuracy: 0.1984\n",
            "Epoch 8/10\n",
            "190/190 [==============================] - 12s 62ms/step - loss: -42148368.0000 - accuracy: 0.1943 - val_loss: -50281112.0000 - val_accuracy: 0.1984\n",
            "Epoch 9/10\n",
            "190/190 [==============================] - 9s 45ms/step - loss: -66373884.0000 - accuracy: 0.1943 - val_loss: -76820976.0000 - val_accuracy: 0.1984\n",
            "Epoch 10/10\n",
            "190/190 [==============================] - 12s 61ms/step - loss: -98758528.0000 - accuracy: 0.1943 - val_loss: -111733368.0000 - val_accuracy: 0.1984\n",
            "27/27 [==============================] - 0s 10ms/step - loss: -120188968.0000 - accuracy: 0.1958\n",
            "Test score: -120188968.0\n",
            "Test accuracy: 0.19577960669994354\n",
            "27/27 [==============================] - 0s 10ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        43\n",
            "           1       0.20      1.00      0.33       167\n",
            "           2       0.00      0.00      0.00       139\n",
            "           3       0.00      0.00      0.00       182\n",
            "           4       0.00      0.00      0.00       173\n",
            "           5       0.00      0.00      0.00       149\n",
            "\n",
            "    accuracy                           0.20       853\n",
            "   macro avg       0.03      0.17      0.05       853\n",
            "weighted avg       0.04      0.20      0.06       853\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Preprocessing text data\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
        "model = transformers.TFAutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Preprocessing text data\n",
        "X_train, y_train = getXY(train_df)\n",
        "X_test, y_test = getXY(test_df)\n",
        "\n",
        "# Tokenizing the input data\n",
        "max_length = 100\n",
        "\n",
        "train_tokens = tokenizer.batch_encode_plus(\n",
        "    X_train.tolist(),\n",
        "    max_length=max_length,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "test_tokens = tokenizer.batch_encode_plus(\n",
        "    X_test.tolist(),\n",
        "    max_length=max_length,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# Converting the input data to tensors\n",
        "train_seq = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_tokens),\n",
        "    y_train\n",
        ")).batch(32)\n",
        "test_seq = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_tokens),\n",
        "    y_test\n",
        ")).batch(32)\n",
        "\n",
        "# Building a custom classification layer on top of the pre-trained BERT model\n",
        "input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\n",
        "attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')\n",
        "\n",
        "embeddings = model.bert(input_ids, attention_mask=attention_mask)[1]\n",
        "dense = tf.keras.layers.Dense(128, activation='relu')(embeddings)\n",
        "dropout = tf.keras.layers.Dropout(0.3)(dense)\n",
        "output = tf.keras.layers.Dense(1, activation='sigmoid')(dropout)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
        "\n",
        "# Compiling and training the model\n",
        "optimizer = tf.keras.optimizers.Adam(lr=2e-5)\n",
        "classifier.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy', f1_metric])\n",
        "\n",
        "history = classifier.fit(\n",
        "    train_seq,\n",
        "    validation_data=test_seq,\n",
        "    epochs=3,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluating the model on the test set\n",
        "_, acc = classifier.evaluate(test_seq)\n",
        "print('Test accuracy:', acc)\n",
        "test_preds = np.round(classifier.predict(test_seq))\n",
        "print(classification_report(y_test, test_preds))"
      ],
      "metadata": {
        "id": "vnU4bi07htD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLb2EuJ2D6CE",
        "outputId": "4e74b460-c895-4c81-ae8a-0e43845ed9e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "190/190 [==============================] - 16s 77ms/step - loss: -1558.3479 - accuracy: 0.1943 - val_loss: -8235.5586 - val_accuracy: 0.1984\n",
            "Epoch 2/30\n",
            "190/190 [==============================] - 14s 73ms/step - loss: -65936.7422 - accuracy: 0.1943 - val_loss: -174187.7656 - val_accuracy: 0.1984\n",
            "Epoch 3/30\n",
            "190/190 [==============================] - 13s 70ms/step - loss: -514772.1250 - accuracy: 0.1943 - val_loss: -966493.6875 - val_accuracy: 0.1984\n",
            "Epoch 4/30\n",
            "190/190 [==============================] - 13s 68ms/step - loss: -1985614.2500 - accuracy: 0.1943 - val_loss: -3105717.5000 - val_accuracy: 0.1984\n",
            "Epoch 5/30\n",
            "190/190 [==============================] - 10s 55ms/step - loss: -5315688.5000 - accuracy: 0.1943 - val_loss: -7430963.0000 - val_accuracy: 0.1984\n",
            "Epoch 6/30\n",
            "190/190 [==============================] - 13s 66ms/step - loss: -11490533.0000 - accuracy: 0.1943 - val_loss: -14950065.0000 - val_accuracy: 0.1984\n",
            "Epoch 7/30\n",
            "190/190 [==============================] - 13s 67ms/step - loss: -21526586.0000 - accuracy: 0.1943 - val_loss: -26645846.0000 - val_accuracy: 0.1984\n",
            "Epoch 8/30\n",
            "190/190 [==============================] - 13s 70ms/step - loss: -36463844.0000 - accuracy: 0.1943 - val_loss: -43468800.0000 - val_accuracy: 0.1984\n",
            "Epoch 9/30\n",
            "190/190 [==============================] - 11s 58ms/step - loss: -57401960.0000 - accuracy: 0.1943 - val_loss: -66502392.0000 - val_accuracy: 0.1984\n",
            "Epoch 10/30\n",
            "190/190 [==============================] - 12s 62ms/step - loss: -85387056.0000 - accuracy: 0.1943 - val_loss: -96652696.0000 - val_accuracy: 0.1984\n",
            "Epoch 11/30\n",
            "190/190 [==============================] - 12s 66ms/step - loss: -121549096.0000 - accuracy: 0.1943 - val_loss: -135018608.0000 - val_accuracy: 0.1984\n",
            "Epoch 12/30\n",
            "190/190 [==============================] - 15s 76ms/step - loss: -166959232.0000 - accuracy: 0.1943 - val_loss: -182915984.0000 - val_accuracy: 0.1984\n",
            "Epoch 13/30\n",
            "190/190 [==============================] - 12s 61ms/step - loss: -222696704.0000 - accuracy: 0.1943 - val_loss: -240788912.0000 - val_accuracy: 0.1984\n",
            "Epoch 14/30\n",
            "190/190 [==============================] - 13s 66ms/step - loss: -289835520.0000 - accuracy: 0.1943 - val_loss: -310212832.0000 - val_accuracy: 0.1984\n",
            "Epoch 15/30\n",
            "190/190 [==============================] - 13s 67ms/step - loss: -369272992.0000 - accuracy: 0.1943 - val_loss: -391632320.0000 - val_accuracy: 0.1984\n",
            "Epoch 16/30\n",
            "190/190 [==============================] - 11s 59ms/step - loss: -462153088.0000 - accuracy: 0.1943 - val_loss: -486020416.0000 - val_accuracy: 0.1984\n",
            "Epoch 17/30\n",
            "190/190 [==============================] - 11s 59ms/step - loss: -569562304.0000 - accuracy: 0.1943 - val_loss: -594845312.0000 - val_accuracy: 0.1984\n",
            "Epoch 18/30\n",
            "190/190 [==============================] - 13s 67ms/step - loss: -692223424.0000 - accuracy: 0.1943 - val_loss: -718638912.0000 - val_accuracy: 0.1984\n",
            "Epoch 19/30\n",
            "190/190 [==============================] - 13s 67ms/step - loss: -831498816.0000 - accuracy: 0.1943 - val_loss: -858477120.0000 - val_accuracy: 0.1984\n",
            "Epoch 20/30\n",
            "190/190 [==============================] - 12s 61ms/step - loss: -988420608.0000 - accuracy: 0.1943 - val_loss: -1015541632.0000 - val_accuracy: 0.1984\n",
            "Epoch 21/30\n",
            "190/190 [==============================] - 11s 59ms/step - loss: -1163446272.0000 - accuracy: 0.1943 - val_loss: -1189863296.0000 - val_accuracy: 0.1984\n",
            "Epoch 22/30\n",
            "190/190 [==============================] - 13s 67ms/step - loss: -1357847040.0000 - accuracy: 0.1943 - val_loss: -1383651840.0000 - val_accuracy: 0.1984\n",
            "Epoch 23/30\n",
            "190/190 [==============================] - 15s 78ms/step - loss: -1573088128.0000 - accuracy: 0.1943 - val_loss: -1597663488.0000 - val_accuracy: 0.1984\n",
            "Epoch 24/30\n",
            "190/190 [==============================] - 12s 61ms/step - loss: -1809530240.0000 - accuracy: 0.1943 - val_loss: -1831552768.0000 - val_accuracy: 0.1984\n",
            "Epoch 25/30\n",
            "190/190 [==============================] - 12s 62ms/step - loss: -2068717568.0000 - accuracy: 0.1943 - val_loss: -2088149888.0000 - val_accuracy: 0.1984\n",
            "Epoch 26/30\n",
            "190/190 [==============================] - 13s 67ms/step - loss: -2351835136.0000 - accuracy: 0.1943 - val_loss: -2366891520.0000 - val_accuracy: 0.1984\n",
            "Epoch 27/30\n",
            "190/190 [==============================] - 13s 67ms/step - loss: -2659902976.0000 - accuracy: 0.1943 - val_loss: -2670469632.0000 - val_accuracy: 0.1984\n",
            "Epoch 28/30\n",
            "190/190 [==============================] - 11s 60ms/step - loss: -2993818880.0000 - accuracy: 0.1943 - val_loss: -2999879936.0000 - val_accuracy: 0.1984\n",
            "Epoch 29/30\n",
            "190/190 [==============================] - 11s 60ms/step - loss: -3353935360.0000 - accuracy: 0.1943 - val_loss: -3353156608.0000 - val_accuracy: 0.1984\n",
            "Epoch 30/30\n",
            "190/190 [==============================] - 13s 67ms/step - loss: -3741894144.0000 - accuracy: 0.1943 - val_loss: -3733356288.0000 - val_accuracy: 0.1984\n",
            "27/27 [==============================] - 0s 10ms/step - loss: -4019812608.0000 - accuracy: 0.1958\n",
            "Test score: -4019812608.0\n",
            "Test accuracy: 0.19577960669994354\n",
            "27/27 [==============================] - 0s 10ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        43\n",
            "           1       0.20      1.00      0.33       167\n",
            "           2       0.00      0.00      0.00       139\n",
            "           3       0.00      0.00      0.00       182\n",
            "           4       0.00      0.00      0.00       173\n",
            "           5       0.00      0.00      0.00       149\n",
            "\n",
            "    accuracy                           0.20       853\n",
            "   macro avg       0.03      0.17      0.05       853\n",
            "weighted avg       0.04      0.20      0.06       853\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# For Hybrid Model : Statement + Speaker\n",
        "# Concatenate 'statement' and 'speaker' columns and provide it as input\n",
        "def getHybridXY1(data): \n",
        "  y = data['label'].values\n",
        "  X = data['statement'].values+data['speaker'].values\n",
        "  for i in range(len(X)):\n",
        "    X[i] = preprocess(X[i])\n",
        "  return X,y\n",
        "\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
        "model = transformers.TFAutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Preprocessing text data\n",
        "X_train, y_train = getHybridXY(train_df)\n",
        "X_test, y_test = getHybridXY(test_df)\n",
        "\n",
        "# Tokenizing the input data\n",
        "max_length = 100\n",
        "\n",
        "train_tokens = tokenizer.batch_encode_plus(\n",
        "    X_train.tolist(),\n",
        "    max_length=max_length,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "test_tokens = tokenizer.batch_encode_plus(\n",
        "    X_test.tolist(),\n",
        "    max_length=max_length,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# Converting the input data to tensors\n",
        "train_seq = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_tokens),\n",
        "    y_train\n",
        ")).batch(32)\n",
        "test_seq = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_tokens),\n",
        "    y_test\n",
        ")).batch(32)\n",
        "\n",
        "# Building a custom classification layer on top of the pre-trained BERT model\n",
        "input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\n",
        "attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')\n",
        "\n",
        "embeddings = model.bert(input_ids, attention_mask=attention_mask)[1]\n",
        "dense = tf.keras.layers.Dense(128, activation='relu')(embeddings)\n",
        "dropout = tf.keras.layers.Dropout(0.3)(dense)\n",
        "output = tf.keras.layers.Dense(1, activation='sigmoid')(dropout)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
        "\n",
        "# Compiling and training the model\n",
        "optimizer = tf.keras.optimizers.Adam(lr=2e-5)\n",
        "classifier.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy', f1_metric])\n",
        "\n",
        "history = classifier.fit(\n",
        "    train_seq,\n",
        "    validation_data=test_seq,\n",
        "    epochs=3,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluating the model on the test set\n",
        "_, acc = classifier.evaluate(test_seq)\n",
        "print('Test accuracy:', acc)\n",
        "test_preds = np.round(classifier.predict(test_seq))\n",
        "print(classification_report(y_test, test_preds))"
      ],
      "metadata": {
        "id": "2P1AfZQFhqEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Or_YB7OtfT8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0224a619-32e5-494c-c23a-d381317ce2ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "190/190 [==============================] - 13s 66ms/step - loss: -1123.4097 - accuracy: 0.1938 - val_loss: -6302.8076 - val_accuracy: 0.1984\n",
            "Epoch 2/10\n",
            "190/190 [==============================] - 11s 60ms/step - loss: -56889.8555 - accuracy: 0.1943 - val_loss: -150741.5625 - val_accuracy: 0.1984\n",
            "Epoch 3/10\n",
            "190/190 [==============================] - 14s 74ms/step - loss: -452400.5625 - accuracy: 0.1943 - val_loss: -851753.3750 - val_accuracy: 0.1984\n",
            "Epoch 4/10\n",
            "190/190 [==============================] - 11s 59ms/step - loss: -1755536.7500 - accuracy: 0.1943 - val_loss: -2750540.0000 - val_accuracy: 0.1984\n",
            "Epoch 5/10\n",
            "190/190 [==============================] - 12s 65ms/step - loss: -4730885.5000 - accuracy: 0.1943 - val_loss: -6651749.5000 - val_accuracy: 0.1984\n",
            "Epoch 6/10\n",
            "190/190 [==============================] - 12s 63ms/step - loss: -10267682.0000 - accuracy: 0.1943 - val_loss: -13393136.0000 - val_accuracy: 0.1984\n",
            "Epoch 7/10\n",
            "190/190 [==============================] - 10s 50ms/step - loss: -19307746.0000 - accuracy: 0.1943 - val_loss: -23919488.0000 - val_accuracy: 0.1984\n",
            "Epoch 8/10\n",
            "190/190 [==============================] - 12s 64ms/step - loss: -32815634.0000 - accuracy: 0.1943 - val_loss: -39148684.0000 - val_accuracy: 0.1984\n",
            "Epoch 9/10\n",
            "190/190 [==============================] - 12s 63ms/step - loss: -51673736.0000 - accuracy: 0.1943 - val_loss: -59884564.0000 - val_accuracy: 0.1984\n",
            "Epoch 10/10\n",
            "190/190 [==============================] - 15s 78ms/step - loss: -76810304.0000 - accuracy: 0.1943 - val_loss: -86934928.0000 - val_accuracy: 0.1984\n",
            "27/27 [==============================] - 1s 20ms/step - loss: -93512648.0000 - accuracy: 0.1958\n",
            "Test score: -93512648.0\n",
            "Test accuracy: 0.19577960669994354\n",
            "27/27 [==============================] - 1s 18ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        43\n",
            "           1       0.20      1.00      0.33       167\n",
            "           2       0.00      0.00      0.00       139\n",
            "           3       0.00      0.00      0.00       182\n",
            "           4       0.00      0.00      0.00       173\n",
            "           5       0.00      0.00      0.00       149\n",
            "\n",
            "    accuracy                           0.20       853\n",
            "   macro avg       0.03      0.17      0.05       853\n",
            "weighted avg       0.04      0.20      0.06       853\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# For Hybrid Model : Statement + Subject\n",
        "# Concatenate 'statement' and 'subject' columns and provide it as input\n",
        "def getHybridXY(data): \n",
        "  y = data['label'].values\n",
        "  X = data['statement'].values+data['subject'].values\n",
        "  for i in range(len(X)):\n",
        "    X[i] = preprocess(X[i])\n",
        "  return X,y\n",
        "\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
        "model = transformers.TFAutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Preprocessing text data\n",
        "X_train, y_train = getHybridXY(train_df)\n",
        "X_test, y_test = getHybridXY(test_df)\n",
        "\n",
        "# Tokenizing the input data\n",
        "max_length = 100\n",
        "\n",
        "train_tokens = tokenizer.batch_encode_plus(\n",
        "    X_train.tolist(),\n",
        "    max_length=max_length,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "test_tokens = tokenizer.batch_encode_plus(\n",
        "    X_test.tolist(),\n",
        "    max_length=max_length,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# Converting the input data to tensors\n",
        "train_seq = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_tokens),\n",
        "    y_train\n",
        ")).batch(32)\n",
        "test_seq = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_tokens),\n",
        "    y_test\n",
        ")).batch(32)\n",
        "\n",
        "# Building a custom classification layer on top of the pre-trained BERT model\n",
        "input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\n",
        "attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')\n",
        "\n",
        "embeddings = model.bert(input_ids, attention_mask=attention_mask)[1]\n",
        "dense = tf.keras.layers.Dense(128, activation='relu')(embeddings)\n",
        "dropout = tf.keras.layers.Dropout(0.3)(dense)\n",
        "output = tf.keras.layers.Dense(1, activation='sigmoid')(dropout)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
        "\n",
        "# Compiling and training the model\n",
        "optimizer = tf.keras.optimizers.Adam(lr=2e-5)\n",
        "classifier.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy', f1_metric])\n",
        "\n",
        "history = classifier.fit(\n",
        "    train_seq,\n",
        "    validation_data=test_seq,\n",
        "    epochs=3,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluating the model on the test set\n",
        "_, acc = classifier.evaluate(test_seq)\n",
        "print('Test accuracy:', acc)\n",
        "test_preds = np.round(classifier.predict(test_seq))\n",
        "print(classification_report(y_test, test_preds))"
      ],
      "metadata": {
        "id": "6b-E1_T7hovG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88Ga7YoDBfgn",
        "outputId": "f1686cf2-3169-4eff-e29e-1fdd29b39d71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "190/190 [==============================] - 14s 66ms/step - loss: -1061.1967 - accuracy: 0.1936 - val_loss: -5418.8311 - val_accuracy: 0.1984\n",
            "Epoch 2/10\n",
            "190/190 [==============================] - 10s 52ms/step - loss: -40651.2070 - accuracy: 0.1943 - val_loss: -104001.8984 - val_accuracy: 0.1984\n",
            "Epoch 3/10\n",
            "190/190 [==============================] - 14s 72ms/step - loss: -301070.6875 - accuracy: 0.1943 - val_loss: -554392.4375 - val_accuracy: 0.1984\n",
            "Epoch 4/10\n",
            "190/190 [==============================] - 13s 66ms/step - loss: -1122751.0000 - accuracy: 0.1943 - val_loss: -1737727.0000 - val_accuracy: 0.1984\n",
            "Epoch 5/10\n",
            "190/190 [==============================] - 12s 64ms/step - loss: -2975887.5000 - accuracy: 0.1943 - val_loss: -4133655.7500 - val_accuracy: 0.1984\n",
            "Epoch 6/10\n",
            "190/190 [==============================] - 11s 56ms/step - loss: -6388657.0000 - accuracy: 0.1943 - val_loss: -8275629.0000 - val_accuracy: 0.1984\n",
            "Epoch 7/10\n",
            "190/190 [==============================] - 11s 60ms/step - loss: -11916079.0000 - accuracy: 0.1943 - val_loss: -14677576.0000 - val_accuracy: 0.1984\n",
            "Epoch 8/10\n",
            "190/190 [==============================] - 12s 63ms/step - loss: -20166424.0000 - accuracy: 0.1943 - val_loss: -23939940.0000 - val_accuracy: 0.1984\n",
            "Epoch 9/10\n",
            "190/190 [==============================] - 12s 66ms/step - loss: -31722354.0000 - accuracy: 0.1943 - val_loss: -36591376.0000 - val_accuracy: 0.1984\n",
            "Epoch 10/10\n",
            "190/190 [==============================] - 10s 52ms/step - loss: -47129348.0000 - accuracy: 0.1943 - val_loss: -53122960.0000 - val_accuracy: 0.1984\n",
            "27/27 [==============================] - 0s 10ms/step - loss: -57279692.0000 - accuracy: 0.1958\n",
            "Test score: -57279692.0\n",
            "Test accuracy: 0.19577960669994354\n",
            "27/27 [==============================] - 0s 9ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        43\n",
            "           1       0.20      1.00      0.33       167\n",
            "           2       0.00      0.00      0.00       139\n",
            "           3       0.00      0.00      0.00       182\n",
            "           4       0.00      0.00      0.00       173\n",
            "           5       0.00      0.00      0.00       149\n",
            "\n",
            "    accuracy                           0.20       853\n",
            "   macro avg       0.03      0.17      0.05       853\n",
            "weighted avg       0.04      0.20      0.06       853\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# For Hybrid Model : Statement + speaker job title\n",
        "# Concatenate 'statement' and 'speaker job title' columns and provide it as input\n",
        "def getHybridXY(data): \n",
        "  y = data['label'].values\n",
        "  X = data['statement'].values+data['speaker job title'].values\n",
        "  for i in range(len(X)):\n",
        "    X[i] = preprocess(X[i])\n",
        "  return X,y\n",
        "\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
        "model = transformers.TFAutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Preprocessing text data\n",
        "X_train, y_train = getHybridXY(train_df)\n",
        "X_test, y_test = getHybridXY(test_df)\n",
        "\n",
        "# Tokenizing the input data\n",
        "max_length = 100\n",
        "\n",
        "train_tokens = tokenizer.batch_encode_plus(\n",
        "    X_train.tolist(),\n",
        "    max_length=max_length,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "test_tokens = tokenizer.batch_encode_plus(\n",
        "    X_test.tolist(),\n",
        "    max_length=max_length,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# Converting the input data to tensors\n",
        "train_seq = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_tokens),\n",
        "    y_train\n",
        ")).batch(32)\n",
        "test_seq = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_tokens),\n",
        "    y_test\n",
        ")).batch(32)\n",
        "\n",
        "# Building a custom classification layer on top of the pre-trained BERT model\n",
        "input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\n",
        "attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')\n",
        "\n",
        "embeddings = model.bert(input_ids, attention_mask=attention_mask)[1]\n",
        "dense = tf.keras.layers.Dense(128, activation='relu')(embeddings)\n",
        "dropout = tf.keras.layers.Dropout(0.3)(dense)\n",
        "output = tf.keras.layers.Dense(1, activation='sigmoid')(dropout)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
        "\n",
        "# Compiling and training the model\n",
        "optimizer = tf.keras.optimizers.Adam(lr=2e-5)\n",
        "classifier.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy', f1_metric])\n",
        "\n",
        "history = classifier.fit(\n",
        "    train_seq,\n",
        "    validation_data=test_seq,\n",
        "    epochs=3,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluating the model on the test set\n",
        "_, acc = classifier.evaluate(test_seq)\n",
        "print('Test accuracy:', acc)\n",
        "test_preds = np.round(classifier.predict(test_seq))\n",
        "print(classification_report(y_test, test_preds))"
      ],
      "metadata": {
        "id": "uQV8wxkIhnOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2v1m5soXBrOi",
        "outputId": "4accb24f-aa3d-40c3-aa3d-a3ce6bc982a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "190/190 [==============================] - 12s 58ms/step - loss: -1698.0819 - accuracy: 0.1940 - val_loss: -8577.8633 - val_accuracy: 0.1984\n",
            "Epoch 2/10\n",
            "190/190 [==============================] - 15s 79ms/step - loss: -63744.8320 - accuracy: 0.1943 - val_loss: -162853.1406 - val_accuracy: 0.1984\n",
            "Epoch 3/10\n",
            "190/190 [==============================] - 12s 64ms/step - loss: -465517.1562 - accuracy: 0.1943 - val_loss: -859246.0000 - val_accuracy: 0.1984\n",
            "Epoch 4/10\n",
            "190/190 [==============================] - 10s 53ms/step - loss: -1738582.8750 - accuracy: 0.1943 - val_loss: -2691579.5000 - val_accuracy: 0.1984\n",
            "Epoch 5/10\n",
            "190/190 [==============================] - 12s 66ms/step - loss: -4581034.0000 - accuracy: 0.1943 - val_loss: -6388207.5000 - val_accuracy: 0.1984\n",
            "Epoch 6/10\n",
            "190/190 [==============================] - 12s 65ms/step - loss: -9790500.0000 - accuracy: 0.1943 - val_loss: -12719079.0000 - val_accuracy: 0.1984\n",
            "Epoch 7/10\n",
            "190/190 [==============================] - 11s 59ms/step - loss: -18229200.0000 - accuracy: 0.1943 - val_loss: -22506214.0000 - val_accuracy: 0.1984\n",
            "Epoch 8/10\n",
            "190/190 [==============================] - 10s 55ms/step - loss: -30738198.0000 - accuracy: 0.1943 - val_loss: -36594760.0000 - val_accuracy: 0.1984\n",
            "Epoch 9/10\n",
            "190/190 [==============================] - 13s 66ms/step - loss: -48187256.0000 - accuracy: 0.1943 - val_loss: -55776184.0000 - val_accuracy: 0.1984\n",
            "Epoch 10/10\n",
            "190/190 [==============================] - 12s 65ms/step - loss: -71459816.0000 - accuracy: 0.1943 - val_loss: -80887880.0000 - val_accuracy: 0.1984\n",
            "27/27 [==============================] - 0s 10ms/step - loss: -87059240.0000 - accuracy: 0.1958\n",
            "Test score: -87059240.0\n",
            "Test accuracy: 0.19577960669994354\n",
            "27/27 [==============================] - 0s 9ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        43\n",
            "           1       0.20      1.00      0.33       167\n",
            "           2       0.00      0.00      0.00       139\n",
            "           3       0.00      0.00      0.00       182\n",
            "           4       0.00      0.00      0.00       173\n",
            "           5       0.00      0.00      0.00       149\n",
            "\n",
            "    accuracy                           0.20       853\n",
            "   macro avg       0.03      0.17      0.05       853\n",
            "weighted avg       0.04      0.20      0.06       853\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# For Hybrid Model : Statement + state info\n",
        "# Concatenate 'statement' and 'state info' columns and provide it as input\n",
        "def getHybridXY(data): \n",
        "  y = data['label'].values\n",
        "  X = data['statement'].values+data['state info'].values\n",
        "  for i in range(len(X)):\n",
        "    X[i] = preprocess(X[i])\n",
        "  return X,y\n",
        "\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
        "model = transformers.TFAutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Preprocessing text data\n",
        "X_train, y_train = getHybridXY(train_df)\n",
        "X_test, y_test = getHybridXY(test_df)\n",
        "\n",
        "# Tokenizing the input data\n",
        "max_length = 100\n",
        "\n",
        "train_tokens = tokenizer.batch_encode_plus(\n",
        "    X_train.tolist(),\n",
        "    max_length=max_length,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "test_tokens = tokenizer.batch_encode_plus(\n",
        "    X_test.tolist(),\n",
        "    max_length=max_length,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# Converting the input data to tensors\n",
        "train_seq = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_tokens),\n",
        "    y_train\n",
        ")).batch(32)\n",
        "test_seq = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_tokens),\n",
        "    y_test\n",
        ")).batch(32)\n",
        "\n",
        "# Building a custom classification layer on top of the pre-trained BERT model\n",
        "input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\n",
        "attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')\n",
        "\n",
        "embeddings = model.bert(input_ids, attention_mask=attention_mask)[1]\n",
        "dense = tf.keras.layers.Dense(128, activation='relu')(embeddings)\n",
        "dropout = tf.keras.layers.Dropout(0.3)(dense)\n",
        "output = tf.keras.layers.Dense(1, activation='sigmoid')(dropout)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
        "\n",
        "# Compiling and training the model\n",
        "optimizer = tf.keras.optimizers.Adam(lr=2e-5)\n",
        "classifier.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy', f1_metric])\n",
        "\n",
        "history = classifier.fit(\n",
        "    train_seq,\n",
        "    validation_data=test_seq,\n",
        "    epochs=3,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluating the model on the test set\n",
        "_, acc = classifier.evaluate(test_seq)\n",
        "print('Test accuracy:', acc)\n",
        "test_preds = np.round(classifier.predict(test_seq))\n",
        "print(classification_report(y_test, test_preds))"
      ],
      "metadata": {
        "id": "kNFmCJsqhl11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25RAyv0GCKRk",
        "outputId": "a457e98b-0f21-4ba9-902f-af18a5bead7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "190/190 [==============================] - 13s 63ms/step - loss: -953.2339 - accuracy: 0.1935 - val_loss: -4612.2568 - val_accuracy: 0.1984\n",
            "Epoch 2/10\n",
            "190/190 [==============================] - 11s 57ms/step - loss: -31876.7656 - accuracy: 0.1943 - val_loss: -80221.2656 - val_accuracy: 0.1984\n",
            "Epoch 3/10\n",
            "190/190 [==============================] - 13s 69ms/step - loss: -225913.1719 - accuracy: 0.1943 - val_loss: -413576.8125 - val_accuracy: 0.1984\n",
            "Epoch 4/10\n",
            "190/190 [==============================] - 12s 62ms/step - loss: -836843.0000 - accuracy: 0.1943 - val_loss: -1290745.6250 - val_accuracy: 0.1984\n",
            "Epoch 5/10\n",
            "190/190 [==============================] - 9s 49ms/step - loss: -2190837.7500 - accuracy: 0.1943 - val_loss: -3043496.0000 - val_accuracy: 0.1984\n",
            "Epoch 6/10\n",
            "190/190 [==============================] - 12s 62ms/step - loss: -4656988.5000 - accuracy: 0.1943 - val_loss: -6032833.5000 - val_accuracy: 0.1984\n",
            "Epoch 7/10\n",
            "190/190 [==============================] - 12s 65ms/step - loss: -8647422.0000 - accuracy: 0.1943 - val_loss: -10671977.0000 - val_accuracy: 0.1984\n",
            "Epoch 8/10\n",
            "190/190 [==============================] - 11s 56ms/step - loss: -14544556.0000 - accuracy: 0.1943 - val_loss: -17290342.0000 - val_accuracy: 0.1984\n",
            "Epoch 9/10\n",
            "190/190 [==============================] - 11s 56ms/step - loss: -22772874.0000 - accuracy: 0.1943 - val_loss: -26322332.0000 - val_accuracy: 0.1984\n",
            "Epoch 10/10\n",
            "190/190 [==============================] - 11s 61ms/step - loss: -33734864.0000 - accuracy: 0.1943 - val_loss: -38148848.0000 - val_accuracy: 0.1984\n",
            "27/27 [==============================] - 0s 10ms/step - loss: -41043268.0000 - accuracy: 0.1958\n",
            "Test score: -41043268.0\n",
            "Test accuracy: 0.19577960669994354\n",
            "27/27 [==============================] - 0s 9ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        43\n",
            "           1       0.20      1.00      0.33       167\n",
            "           2       0.00      0.00      0.00       139\n",
            "           3       0.00      0.00      0.00       182\n",
            "           4       0.00      0.00      0.00       173\n",
            "           5       0.00      0.00      0.00       149\n",
            "\n",
            "    accuracy                           0.20       853\n",
            "   macro avg       0.03      0.17      0.05       853\n",
            "weighted avg       0.04      0.20      0.06       853\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# For Hybrid Model : Statement + party affilation\n",
        "# Concatenate 'statement' and 'party affilation' columns and provide it as input\n",
        "def getHybridXY(data): \n",
        "  y = data['label'].values\n",
        "  X = data['statement'].values+data['party affilation'].values\n",
        "  for i in range(len(X)):\n",
        "    X[i] = preprocess(X[i])\n",
        "  return X,y\n",
        "\n",
        "\n",
        "# Creating a tokenizer to convert text to sequences\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
        "model = transformers.TFAutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Preprocessing text data\n",
        "X_train, y_train = getHybridXY(train_df)\n",
        "X_test, y_test = getHybridXY(test_df)\n",
        "\n",
        "# Tokenizing the input data\n",
        "max_length = 100\n",
        "\n",
        "train_tokens = tokenizer.batch_encode_plus(\n",
        "    X_train.tolist(),\n",
        "    max_length=max_length,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "test_tokens = tokenizer.batch_encode_plus(\n",
        "    X_test.tolist(),\n",
        "    max_length=max_length,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# Converting the input data to tensors\n",
        "train_seq = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_tokens),\n",
        "    y_train\n",
        ")).batch(32)\n",
        "test_seq = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_tokens),\n",
        "    y_test\n",
        ")).batch(32)\n",
        "\n",
        "# Building a custom classification layer on top of the pre-trained BERT model\n",
        "input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\n",
        "attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')\n",
        "\n",
        "embeddings = model.bert(input_ids, attention_mask=attention_mask)[1]\n",
        "dense = tf.keras.layers.Dense(128, activation='relu')(embeddings)\n",
        "dropout = tf.keras.layers.Dropout(0.3)(dense)\n",
        "output = tf.keras.layers.Dense(1, activation='sigmoid')(dropout)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
        "\n",
        "# Compiling and training the model\n",
        "optimizer = tf.keras.optimizers.Adam(lr=2e-5)\n",
        "classifier.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy', f1_metric])\n",
        "\n",
        "history = classifier.fit(\n",
        "    train_seq,\n",
        "    validation_data=test_seq,\n",
        "    epochs=3,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluating the model on the test set\n",
        "_, acc = classifier.evaluate(test_seq)\n",
        "print('Test accuracy:', acc)\n",
        "test_preds = np.round(classifier.predict(test_seq))\n",
        "print(classification_report(y_test, test_preds))"
      ],
      "metadata": {
        "id": "3u3fdTbdhkYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_8U1LTdCqXU"
      },
      "outputs": [],
      "source": [
        "# For Hybrid Model : Statement + location of statement\n",
        "# Concatenate 'statement' and 'location of statement' columns and provide it as input\n",
        "def getHybridXY(data): \n",
        "  y = data['label'].values\n",
        "  X = data['statement'].values+data['location of statement'].values\n",
        "  for i in range(len(X)):\n",
        "    X[i] = preprocess(X[i])\n",
        "  return X,y\n",
        "\n",
        "  # Preprocessing text data\n",
        "X_train, y_train = getHybridXY(train_df)\n",
        "X_test, y_test = getHybridXY(test_df)\n",
        "\n",
        "# Creating a tokenizer to convert text to sequences\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# Converting text to sequences using the tokenizer\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Padding sequences to a fixed length\n",
        "maxlen = 100\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "# Building the CNN model\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 100\n",
        "\n",
        "classifier = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
        "\n",
        "# Compiling and training the model\n",
        "optimizer = tf.keras.optimizers.Adam(lr=2e-5)\n",
        "classifier.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy', fi_metric])\n",
        "\n",
        "history = classifier.fit(\n",
        "    train_seq,\n",
        "    validation_data=test_seq,\n",
        "    epochs=3,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluating the model on the test set\n",
        "_, acc = classifier.evaluate(test_seq)\n",
        "print('Test accuracy:', acc)\n",
        "# test_preds = np.round(classifier.predict(test_seq))\n",
        "# print(classification_report(y_test, test_preds))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "RiqsnGgXDFd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "607b367b-b5f1-4428-a96d-a4d5d8ff1c54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "# Importing necessary libraries\n",
        "import tensorflow as tf\n",
        "import transformers\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "\n",
        "# Loading the pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
        "model = transformers.TFAutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Preprocessing text data\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras.backend as K\n",
        "def f1_metric(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val"
      ],
      "metadata": {
        "id": "6hzsiFpBhzJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = getHybridXY(train_df)\n",
        "X_test, y_test = getHybridXY(test_df)\n",
        "\n",
        "# Tokenizing the input data\n",
        "max_length = 100\n",
        "\n",
        "train_tokens = tokenizer.batch_encode_plus(\n",
        "    X_train.tolist(),\n",
        "    max_length=max_length,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "test_tokens = tokenizer.batch_encode_plus(\n",
        "    X_test.tolist(),\n",
        "    max_length=max_length,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# Converting the input data to tensors\n",
        "train_seq = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_tokens),\n",
        "    y_train\n",
        ")).batch(32)\n",
        "test_seq = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_tokens),\n",
        "    y_test\n",
        ")).batch(32)\n",
        "\n",
        "# Building a custom classification layer on top of the pre-trained BERT model\n",
        "input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\n",
        "attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')\n",
        "\n",
        "embeddings = model.bert(input_ids, attention_mask=attention_mask)[1]\n",
        "dense = tf.keras.layers.Dense(128, activation='relu')(embeddings)\n",
        "dropout = tf.keras.layers.Dropout(0.3)(dense)\n",
        "output = tf.keras.layers.Dense(1, activation='sigmoid')(dropout)"
      ],
      "metadata": {
        "id": "yvOUQf99TCvP"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
        "\n",
        "# Compiling and training the model\n",
        "optimizer = tf.keras.optimizers.Adam(lr=2e-5)\n",
        "classifier.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy', f1_metric])\n",
        "\n",
        "history = classifier.fit(\n",
        "    train_seq,\n",
        "    validation_data=test_seq,\n",
        "    epochs=3,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluating the model on the test set\n",
        "_, acc = classifier.evaluate(test_seq)\n",
        "print('Test accuracy:', acc)\n",
        "test_preds = np.round(classifier.predict(test_seq))\n",
        "print(classification_report(y_test, test_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "C3DJBYJIS-Bv",
        "outputId": "88220d8b-dadc-4815-feb2-cbc1c66f1d2e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            " 21/238 [=>............................] - ETA: 2:36:39 - loss: 0.0000e+00 - accuracy: 0.1949 - f1_metric: 0.9409"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-62a979ab7e76>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_metric\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m history = classifier.fit(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_seq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_seq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}