{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mz9lM0Dq2Ie5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed42f741-7d98-4178-df4f-ac574efa852c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (0.15.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.98-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.11.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.25.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.10.31)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers) (8.1.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->sentence-transformers) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.12)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125942 sha256=449fae0351d10926dadcf1dab5cc900ba145f31e86bf513979f44ea07cc24965\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/67/06/162a3760c40d74dd40bc855d527008d26341c2b0ecf3e8e11f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, huggingface-hub, transformers, sentence-transformers\n",
            "Successfully installed huggingface-hub-0.13.4 sentence-transformers-2.2.2 sentencepiece-0.1.98 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Wno6LzAa2MLB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4514da74-0282-4004-f717-156e509cf93b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Conv1D, GlobalMaxPooling1D\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Dropout, Conv1D, GlobalMaxPooling1D\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import re\n",
        "import glob\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "import sklearn\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
        "from sklearn.preprocessing import Binarizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, losses, models, util\n",
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "from sentence_transformers.readers import InputExample\n",
        "\n",
        "import warnings\n",
        "from wordcloud import STOPWORDS, WordCloud\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "r1l4oDVf2RCq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f8fc1ef-fa36-4f29-bf7c-973f51b00f64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3FPkgFHW2U5K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91ea4328-de41-4052-89c9-4a16139ae67f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Datasets Sem-6/IR Datasets/Project/liar_dataset.zip\n",
            "  inflating: README                  \n",
            "  inflating: test.tsv                \n",
            "  inflating: train.tsv               \n",
            "  inflating: valid.tsv               \n"
          ]
        }
      ],
      "source": [
        "!unzip '/content/drive/MyDrive/Datasets Sem-6/IR Datasets/Project/liar_dataset.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hqd8FKSC2WQY"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('/content/train.tsv',sep='\\t', header = None)\n",
        "test_df = pd.read_csv('/content/test.tsv',sep='\\t', header = None)\n",
        "val_df = pd.read_csv('/content/valid.tsv',sep='\\t', header = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qIWWW5eX2ZQO"
      },
      "outputs": [],
      "source": [
        "train_df = train_df.drop([0, 8, 9, 10, 11, 12], axis = 1)\n",
        "test_df = test_df.drop([0, 8, 9, 10, 11, 12], axis = 1)\n",
        "val_df = val_df.drop([0, 8, 9, 10, 11, 12], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fa7fiiEp2bf2"
      },
      "outputs": [],
      "source": [
        "train_df.columns = ['label', 'statement', 'subject', 'speaker', 'speaker job title', 'state info', 'party affilation', 'location of statement']\n",
        "test_df.columns = ['label', 'statement', 'subject', 'speaker', 'speaker job title', 'state info', 'party affilation', 'location of statement']\n",
        "val_df.columns = ['label', 'statement', 'subject', 'speaker', 'speaker job title', 'state info', 'party affilation', 'location of statement']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8qtTzGjs3S5O"
      },
      "outputs": [],
      "source": [
        "train_df = train_df.dropna()\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "test_df = test_df.dropna()\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "val_df = val_df.dropna()\n",
        "val_df = val_df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BzJNV0mZ3UiC"
      },
      "outputs": [],
      "source": [
        "train_df = pd.concat([train_df, val_df])\n",
        "train_df = train_df.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "q4Vux97b3WWo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f939d48b-9d43-4e19-9712-9eea1d74f593"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data Dimensions - (7585, 9)\n",
            "Testing Data Dimensions - (853, 8)\n"
          ]
        }
      ],
      "source": [
        "print('Training Data Dimensions -', train_df.shape)\n",
        "print('Testing Data Dimensions -', test_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "eKSVBnVy3X55"
      },
      "outputs": [],
      "source": [
        "labels_dict = {'mostly-true':4,'barely-true':2,'half-true':3,'false':1, 'true':5,'pants-fire':0}\n",
        "train_df['label'] = train_df['label'].apply(lambda x: labels_dict[x])\n",
        "test_df['label'] = test_df['label'].apply(lambda x: labels_dict[x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "az8tc1nLBs-V"
      },
      "outputs": [],
      "source": [
        "def preprocess(text):\n",
        "  text = text.lower() # lower - casing the text\n",
        "  text = re.sub('<[^>]*>', ' ', text)\n",
        "  text = re.sub('[\\W]+', ' ', text)\n",
        "  tokenizer = TreebankWordTokenizer()\n",
        "  words = tokenizer.tokenize(text)\n",
        "  text = ' '.join(words)\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  word_tokens = word_tokenize(text)\n",
        "  filtered_sentence = [w for w in word_tokens if not w in stop_words] # removal of stopwords\n",
        "  text = ' '.join(filtered_sentence)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "tsJZNSKcBtQx"
      },
      "outputs": [],
      "source": [
        "# Standard Model['label' --> Target , 'statement' --> input text data]\n",
        "def get_XY(data):\n",
        "  y = data['label'].values\n",
        "  X = data['statement'].values\n",
        "  for i in range(len(X)):\n",
        "    X[i] = preprocess(X[i])\n",
        "  return X,y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UNe2ry0NBnMK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05990c6d-a7af-4725-9c52-99c01fddab56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "190/190 [==============================] - 15s 71ms/step - loss: -2072.8821 - accuracy: 0.1938 - val_loss: -10735.0938 - val_accuracy: 0.1984\n",
            "Epoch 2/10\n",
            "190/190 [==============================] - 16s 83ms/step - loss: -80415.0312 - accuracy: 0.1943 - val_loss: -208881.4062 - val_accuracy: 0.1984\n",
            "Epoch 3/10\n",
            "190/190 [==============================] - 9s 46ms/step - loss: -606200.1250 - accuracy: 0.1943 - val_loss: -1132892.5000 - val_accuracy: 0.1984\n",
            "Epoch 4/10\n",
            "190/190 [==============================] - 11s 60ms/step - loss: -2323729.0000 - accuracy: 0.1943 - val_loss: -3617297.5000 - val_accuracy: 0.1984\n",
            "Epoch 5/10\n",
            "190/190 [==============================] - 11s 61ms/step - loss: -6202850.0000 - accuracy: 0.1943 - val_loss: -8660338.0000 - val_accuracy: 0.1984\n",
            "Epoch 6/10\n",
            "190/190 [==============================] - 9s 45ms/step - loss: -13321475.0000 - accuracy: 0.1943 - val_loss: -17326170.0000 - val_accuracy: 0.1984\n",
            "Epoch 7/10\n",
            "190/190 [==============================] - 11s 60ms/step - loss: -24884830.0000 - accuracy: 0.1943 - val_loss: -30784124.0000 - val_accuracy: 0.1984\n",
            "Epoch 8/10\n",
            "190/190 [==============================] - 12s 62ms/step - loss: -42148368.0000 - accuracy: 0.1943 - val_loss: -50281112.0000 - val_accuracy: 0.1984\n",
            "Epoch 9/10\n",
            "190/190 [==============================] - 9s 45ms/step - loss: -66373884.0000 - accuracy: 0.1943 - val_loss: -76820976.0000 - val_accuracy: 0.1984\n",
            "Epoch 10/10\n",
            "190/190 [==============================] - 12s 61ms/step - loss: -98758528.0000 - accuracy: 0.1943 - val_loss: -111733368.0000 - val_accuracy: 0.1984\n",
            "27/27 [==============================] - 0s 10ms/step - loss: -120188968.0000 - accuracy: 0.1958\n",
            "Test score: -120188968.0\n",
            "Test accuracy: 0.19577960669994354\n",
            "27/27 [==============================] - 0s 10ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        43\n",
            "           1       0.20      1.00      0.33       167\n",
            "           2       0.00      0.00      0.00       139\n",
            "           3       0.00      0.00      0.00       182\n",
            "           4       0.00      0.00      0.00       173\n",
            "           5       0.00      0.00      0.00       149\n",
            "\n",
            "    accuracy                           0.20       853\n",
            "   macro avg       0.03      0.17      0.05       853\n",
            "weighted avg       0.04      0.20      0.06       853\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Preprocessing text data\n",
        "X_train, y_train = get_XY(train_df)\n",
        "X_test, y_test = get_XY(test_df)\n",
        "\n",
        "# Creating a tokenizer to convert text to sequences\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# Converting text to sequences using the tokenizer\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Padding sequences to a fixed length\n",
        "maxlen = 100\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "# Building the CNN model\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen))\n",
        "model.add(Conv1D(128, 5, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "history = model.fit(X_train, y_train, epochs=10, validation_split=0.2)\n",
        "\n",
        "# Evaluating the model on the test set\n",
        "score, acc = model.evaluate(X_test, y_test)\n",
        "\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)\n",
        "test_preds = model.predict(X_test)\n",
        "print(classification_report(y_test, test_preds))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLb2EuJ2D6CE",
        "outputId": "4e74b460-c895-4c81-ae8a-0e43845ed9e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "190/190 [==============================] - 16s 77ms/step - loss: -1558.3479 - accuracy: 0.1943 - val_loss: -8235.5586 - val_accuracy: 0.1984\n",
            "Epoch 2/30\n",
            "190/190 [==============================] - 14s 73ms/step - loss: -65936.7422 - accuracy: 0.1943 - val_loss: -174187.7656 - val_accuracy: 0.1984\n",
            "Epoch 3/30\n",
            "190/190 [==============================] - 13s 70ms/step - loss: -514772.1250 - accuracy: 0.1943 - val_loss: -966493.6875 - val_accuracy: 0.1984\n",
            "Epoch 4/30\n",
            "190/190 [==============================] - 13s 68ms/step - loss: -1985614.2500 - accuracy: 0.1943 - val_loss: -3105717.5000 - val_accuracy: 0.1984\n",
            "Epoch 5/30\n",
            "190/190 [==============================] - 10s 55ms/step - loss: -5315688.5000 - accuracy: 0.1943 - val_loss: -7430963.0000 - val_accuracy: 0.1984\n",
            "Epoch 6/30\n",
            "190/190 [==============================] - 13s 66ms/step - loss: -11490533.0000 - accuracy: 0.1943 - val_loss: -14950065.0000 - val_accuracy: 0.1984\n",
            "Epoch 7/30\n",
            "190/190 [==============================] - 13s 67ms/step - loss: -21526586.0000 - accuracy: 0.1943 - val_loss: -26645846.0000 - val_accuracy: 0.1984\n",
            "Epoch 8/30\n",
            "190/190 [==============================] - 13s 70ms/step - loss: -36463844.0000 - accuracy: 0.1943 - val_loss: -43468800.0000 - val_accuracy: 0.1984\n",
            "Epoch 9/30\n",
            "190/190 [==============================] - 11s 58ms/step - loss: -57401960.0000 - accuracy: 0.1943 - val_loss: -66502392.0000 - val_accuracy: 0.1984\n",
            "Epoch 10/30\n",
            "190/190 [==============================] - 12s 62ms/step - loss: -85387056.0000 - accuracy: 0.1943 - val_loss: -96652696.0000 - val_accuracy: 0.1984\n",
            "Epoch 11/30\n",
            "190/190 [==============================] - 12s 66ms/step - loss: -121549096.0000 - accuracy: 0.1943 - val_loss: -135018608.0000 - val_accuracy: 0.1984\n",
            "Epoch 12/30\n",
            "190/190 [==============================] - 15s 76ms/step - loss: -166959232.0000 - accuracy: 0.1943 - val_loss: -182915984.0000 - val_accuracy: 0.1984\n",
            "Epoch 13/30\n",
            "190/190 [==============================] - 12s 61ms/step - loss: -222696704.0000 - accuracy: 0.1943 - val_loss: -240788912.0000 - val_accuracy: 0.1984\n",
            "Epoch 14/30\n",
            "190/190 [==============================] - 13s 66ms/step - loss: -289835520.0000 - accuracy: 0.1943 - val_loss: -310212832.0000 - val_accuracy: 0.1984\n",
            "Epoch 15/30\n",
            "190/190 [==============================] - 13s 67ms/step - loss: -369272992.0000 - accuracy: 0.1943 - val_loss: -391632320.0000 - val_accuracy: 0.1984\n",
            "Epoch 16/30\n",
            "190/190 [==============================] - 11s 59ms/step - loss: -462153088.0000 - accuracy: 0.1943 - val_loss: -486020416.0000 - val_accuracy: 0.1984\n",
            "Epoch 17/30\n",
            "190/190 [==============================] - 11s 59ms/step - loss: -569562304.0000 - accuracy: 0.1943 - val_loss: -594845312.0000 - val_accuracy: 0.1984\n",
            "Epoch 18/30\n",
            "190/190 [==============================] - 13s 67ms/step - loss: -692223424.0000 - accuracy: 0.1943 - val_loss: -718638912.0000 - val_accuracy: 0.1984\n",
            "Epoch 19/30\n",
            "190/190 [==============================] - 13s 67ms/step - loss: -831498816.0000 - accuracy: 0.1943 - val_loss: -858477120.0000 - val_accuracy: 0.1984\n",
            "Epoch 20/30\n",
            "190/190 [==============================] - 12s 61ms/step - loss: -988420608.0000 - accuracy: 0.1943 - val_loss: -1015541632.0000 - val_accuracy: 0.1984\n",
            "Epoch 21/30\n",
            "190/190 [==============================] - 11s 59ms/step - loss: -1163446272.0000 - accuracy: 0.1943 - val_loss: -1189863296.0000 - val_accuracy: 0.1984\n",
            "Epoch 22/30\n",
            "190/190 [==============================] - 13s 67ms/step - loss: -1357847040.0000 - accuracy: 0.1943 - val_loss: -1383651840.0000 - val_accuracy: 0.1984\n",
            "Epoch 23/30\n",
            "190/190 [==============================] - 15s 78ms/step - loss: -1573088128.0000 - accuracy: 0.1943 - val_loss: -1597663488.0000 - val_accuracy: 0.1984\n",
            "Epoch 24/30\n",
            "190/190 [==============================] - 12s 61ms/step - loss: -1809530240.0000 - accuracy: 0.1943 - val_loss: -1831552768.0000 - val_accuracy: 0.1984\n",
            "Epoch 25/30\n",
            "190/190 [==============================] - 12s 62ms/step - loss: -2068717568.0000 - accuracy: 0.1943 - val_loss: -2088149888.0000 - val_accuracy: 0.1984\n",
            "Epoch 26/30\n",
            "190/190 [==============================] - 13s 67ms/step - loss: -2351835136.0000 - accuracy: 0.1943 - val_loss: -2366891520.0000 - val_accuracy: 0.1984\n",
            "Epoch 27/30\n",
            "190/190 [==============================] - 13s 67ms/step - loss: -2659902976.0000 - accuracy: 0.1943 - val_loss: -2670469632.0000 - val_accuracy: 0.1984\n",
            "Epoch 28/30\n",
            "190/190 [==============================] - 11s 60ms/step - loss: -2993818880.0000 - accuracy: 0.1943 - val_loss: -2999879936.0000 - val_accuracy: 0.1984\n",
            "Epoch 29/30\n",
            "190/190 [==============================] - 11s 60ms/step - loss: -3353935360.0000 - accuracy: 0.1943 - val_loss: -3353156608.0000 - val_accuracy: 0.1984\n",
            "Epoch 30/30\n",
            "190/190 [==============================] - 13s 67ms/step - loss: -3741894144.0000 - accuracy: 0.1943 - val_loss: -3733356288.0000 - val_accuracy: 0.1984\n",
            "27/27 [==============================] - 0s 10ms/step - loss: -4019812608.0000 - accuracy: 0.1958\n",
            "Test score: -4019812608.0\n",
            "Test accuracy: 0.19577960669994354\n",
            "27/27 [==============================] - 0s 10ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        43\n",
            "           1       0.20      1.00      0.33       167\n",
            "           2       0.00      0.00      0.00       139\n",
            "           3       0.00      0.00      0.00       182\n",
            "           4       0.00      0.00      0.00       173\n",
            "           5       0.00      0.00      0.00       149\n",
            "\n",
            "    accuracy                           0.20       853\n",
            "   macro avg       0.03      0.17      0.05       853\n",
            "weighted avg       0.04      0.20      0.06       853\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# For Hybrid Model : Statement + Speaker\n",
        "# Concatenate 'statement' and 'speaker' columns and provide it as input\n",
        "def getHybridXY1(data): \n",
        "  y = data['label'].values\n",
        "  X = data['statement'].values+data['speaker'].values\n",
        "  for i in range(len(X)):\n",
        "    X[i] = preprocess(X[i])\n",
        "  return X,y\n",
        "\n",
        "X_train, y_train = getHybridXY1(train_df)\n",
        "X_test, y_test = getHybridXY1(test_df)\n",
        "\n",
        "# Creating a tokenizer to convert text to sequences\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# Converting text to sequences using the tokenizer\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Padding sequences to a fixed length\n",
        "maxlen = 100\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "# Building the CNN model\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen))\n",
        "model.add(Conv1D(128, 5, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "history = model.fit(X_train, y_train, epochs=30, validation_split=0.2)\n",
        "\n",
        "# Evaluating the model on the test set\n",
        "score, acc = model.evaluate(X_test, y_test)\n",
        "\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)\n",
        "test_preds = model.predict(X_test)\n",
        "print(classification_report(y_test, test_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_Or_YB7OtfT8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0224a619-32e5-494c-c23a-d381317ce2ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "190/190 [==============================] - 13s 66ms/step - loss: -1123.4097 - accuracy: 0.1938 - val_loss: -6302.8076 - val_accuracy: 0.1984\n",
            "Epoch 2/10\n",
            "190/190 [==============================] - 11s 60ms/step - loss: -56889.8555 - accuracy: 0.1943 - val_loss: -150741.5625 - val_accuracy: 0.1984\n",
            "Epoch 3/10\n",
            "190/190 [==============================] - 14s 74ms/step - loss: -452400.5625 - accuracy: 0.1943 - val_loss: -851753.3750 - val_accuracy: 0.1984\n",
            "Epoch 4/10\n",
            "190/190 [==============================] - 11s 59ms/step - loss: -1755536.7500 - accuracy: 0.1943 - val_loss: -2750540.0000 - val_accuracy: 0.1984\n",
            "Epoch 5/10\n",
            "190/190 [==============================] - 12s 65ms/step - loss: -4730885.5000 - accuracy: 0.1943 - val_loss: -6651749.5000 - val_accuracy: 0.1984\n",
            "Epoch 6/10\n",
            "190/190 [==============================] - 12s 63ms/step - loss: -10267682.0000 - accuracy: 0.1943 - val_loss: -13393136.0000 - val_accuracy: 0.1984\n",
            "Epoch 7/10\n",
            "190/190 [==============================] - 10s 50ms/step - loss: -19307746.0000 - accuracy: 0.1943 - val_loss: -23919488.0000 - val_accuracy: 0.1984\n",
            "Epoch 8/10\n",
            "190/190 [==============================] - 12s 64ms/step - loss: -32815634.0000 - accuracy: 0.1943 - val_loss: -39148684.0000 - val_accuracy: 0.1984\n",
            "Epoch 9/10\n",
            "190/190 [==============================] - 12s 63ms/step - loss: -51673736.0000 - accuracy: 0.1943 - val_loss: -59884564.0000 - val_accuracy: 0.1984\n",
            "Epoch 10/10\n",
            "190/190 [==============================] - 15s 78ms/step - loss: -76810304.0000 - accuracy: 0.1943 - val_loss: -86934928.0000 - val_accuracy: 0.1984\n",
            "27/27 [==============================] - 1s 20ms/step - loss: -93512648.0000 - accuracy: 0.1958\n",
            "Test score: -93512648.0\n",
            "Test accuracy: 0.19577960669994354\n",
            "27/27 [==============================] - 1s 18ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        43\n",
            "           1       0.20      1.00      0.33       167\n",
            "           2       0.00      0.00      0.00       139\n",
            "           3       0.00      0.00      0.00       182\n",
            "           4       0.00      0.00      0.00       173\n",
            "           5       0.00      0.00      0.00       149\n",
            "\n",
            "    accuracy                           0.20       853\n",
            "   macro avg       0.03      0.17      0.05       853\n",
            "weighted avg       0.04      0.20      0.06       853\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# For Hybrid Model : Statement + Subject\n",
        "# Concatenate 'statement' and 'subject' columns and provide it as input\n",
        "def getHybridXY(data): \n",
        "  y = data['label'].values\n",
        "  X = data['statement'].values+data['subject'].values\n",
        "  for i in range(len(X)):\n",
        "    X[i] = preprocess(X[i])\n",
        "  return X,y\n",
        "\n",
        "  # Preprocessing text data\n",
        "X_train, y_train = getHybridXY(train_df)\n",
        "X_test, y_test = getHybridXY(test_df)\n",
        "\n",
        "# Creating a tokenizer to convert text to sequences\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# Converting text to sequences using the tokenizer\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Padding sequences to a fixed length\n",
        "maxlen = 100\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "# Building the CNN model\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen))\n",
        "model.add(Conv1D(128, 5, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "history = model.fit(X_train, y_train, epochs=10, validation_split=0.2)\n",
        "\n",
        "# Evaluating the model on the test set\n",
        "score, acc = model.evaluate(X_test, y_test)\n",
        "\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)\n",
        "test_preds = model.predict(X_test)\n",
        "print(classification_report(y_test, test_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88Ga7YoDBfgn",
        "outputId": "f1686cf2-3169-4eff-e29e-1fdd29b39d71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "190/190 [==============================] - 14s 66ms/step - loss: -1061.1967 - accuracy: 0.1936 - val_loss: -5418.8311 - val_accuracy: 0.1984\n",
            "Epoch 2/10\n",
            "190/190 [==============================] - 10s 52ms/step - loss: -40651.2070 - accuracy: 0.1943 - val_loss: -104001.8984 - val_accuracy: 0.1984\n",
            "Epoch 3/10\n",
            "190/190 [==============================] - 14s 72ms/step - loss: -301070.6875 - accuracy: 0.1943 - val_loss: -554392.4375 - val_accuracy: 0.1984\n",
            "Epoch 4/10\n",
            "190/190 [==============================] - 13s 66ms/step - loss: -1122751.0000 - accuracy: 0.1943 - val_loss: -1737727.0000 - val_accuracy: 0.1984\n",
            "Epoch 5/10\n",
            "190/190 [==============================] - 12s 64ms/step - loss: -2975887.5000 - accuracy: 0.1943 - val_loss: -4133655.7500 - val_accuracy: 0.1984\n",
            "Epoch 6/10\n",
            "190/190 [==============================] - 11s 56ms/step - loss: -6388657.0000 - accuracy: 0.1943 - val_loss: -8275629.0000 - val_accuracy: 0.1984\n",
            "Epoch 7/10\n",
            "190/190 [==============================] - 11s 60ms/step - loss: -11916079.0000 - accuracy: 0.1943 - val_loss: -14677576.0000 - val_accuracy: 0.1984\n",
            "Epoch 8/10\n",
            "190/190 [==============================] - 12s 63ms/step - loss: -20166424.0000 - accuracy: 0.1943 - val_loss: -23939940.0000 - val_accuracy: 0.1984\n",
            "Epoch 9/10\n",
            "190/190 [==============================] - 12s 66ms/step - loss: -31722354.0000 - accuracy: 0.1943 - val_loss: -36591376.0000 - val_accuracy: 0.1984\n",
            "Epoch 10/10\n",
            "190/190 [==============================] - 10s 52ms/step - loss: -47129348.0000 - accuracy: 0.1943 - val_loss: -53122960.0000 - val_accuracy: 0.1984\n",
            "27/27 [==============================] - 0s 10ms/step - loss: -57279692.0000 - accuracy: 0.1958\n",
            "Test score: -57279692.0\n",
            "Test accuracy: 0.19577960669994354\n",
            "27/27 [==============================] - 0s 9ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        43\n",
            "           1       0.20      1.00      0.33       167\n",
            "           2       0.00      0.00      0.00       139\n",
            "           3       0.00      0.00      0.00       182\n",
            "           4       0.00      0.00      0.00       173\n",
            "           5       0.00      0.00      0.00       149\n",
            "\n",
            "    accuracy                           0.20       853\n",
            "   macro avg       0.03      0.17      0.05       853\n",
            "weighted avg       0.04      0.20      0.06       853\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# For Hybrid Model : Statement + speaker job title\n",
        "# Concatenate 'statement' and 'speaker job title' columns and provide it as input\n",
        "def getHybridXY(data): \n",
        "  y = data['label'].values\n",
        "  X = data['statement'].values+data['speaker job title'].values\n",
        "  for i in range(len(X)):\n",
        "    X[i] = preprocess(X[i])\n",
        "  return X,y\n",
        "\n",
        "  # Preprocessing text data\n",
        "X_train, y_train = getHybridXY(train_df)\n",
        "X_test, y_test = getHybridXY(test_df)\n",
        "\n",
        "# Creating a tokenizer to convert text to sequences\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# Converting text to sequences using the tokenizer\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Padding sequences to a fixed length\n",
        "maxlen = 100\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "# Building the CNN model\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen))\n",
        "model.add(Conv1D(128, 5, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "history = model.fit(X_train, y_train, epochs=10, validation_split=0.2)\n",
        "\n",
        "# Evaluating the model on the test set\n",
        "score, acc = model.evaluate(X_test, y_test)\n",
        "\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)\n",
        "test_preds = model.predict(X_test)\n",
        "print(classification_report(y_test, test_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2v1m5soXBrOi",
        "outputId": "4accb24f-aa3d-40c3-aa3d-a3ce6bc982a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "190/190 [==============================] - 12s 58ms/step - loss: -1698.0819 - accuracy: 0.1940 - val_loss: -8577.8633 - val_accuracy: 0.1984\n",
            "Epoch 2/10\n",
            "190/190 [==============================] - 15s 79ms/step - loss: -63744.8320 - accuracy: 0.1943 - val_loss: -162853.1406 - val_accuracy: 0.1984\n",
            "Epoch 3/10\n",
            "190/190 [==============================] - 12s 64ms/step - loss: -465517.1562 - accuracy: 0.1943 - val_loss: -859246.0000 - val_accuracy: 0.1984\n",
            "Epoch 4/10\n",
            "190/190 [==============================] - 10s 53ms/step - loss: -1738582.8750 - accuracy: 0.1943 - val_loss: -2691579.5000 - val_accuracy: 0.1984\n",
            "Epoch 5/10\n",
            "190/190 [==============================] - 12s 66ms/step - loss: -4581034.0000 - accuracy: 0.1943 - val_loss: -6388207.5000 - val_accuracy: 0.1984\n",
            "Epoch 6/10\n",
            "190/190 [==============================] - 12s 65ms/step - loss: -9790500.0000 - accuracy: 0.1943 - val_loss: -12719079.0000 - val_accuracy: 0.1984\n",
            "Epoch 7/10\n",
            "190/190 [==============================] - 11s 59ms/step - loss: -18229200.0000 - accuracy: 0.1943 - val_loss: -22506214.0000 - val_accuracy: 0.1984\n",
            "Epoch 8/10\n",
            "190/190 [==============================] - 10s 55ms/step - loss: -30738198.0000 - accuracy: 0.1943 - val_loss: -36594760.0000 - val_accuracy: 0.1984\n",
            "Epoch 9/10\n",
            "190/190 [==============================] - 13s 66ms/step - loss: -48187256.0000 - accuracy: 0.1943 - val_loss: -55776184.0000 - val_accuracy: 0.1984\n",
            "Epoch 10/10\n",
            "190/190 [==============================] - 12s 65ms/step - loss: -71459816.0000 - accuracy: 0.1943 - val_loss: -80887880.0000 - val_accuracy: 0.1984\n",
            "27/27 [==============================] - 0s 10ms/step - loss: -87059240.0000 - accuracy: 0.1958\n",
            "Test score: -87059240.0\n",
            "Test accuracy: 0.19577960669994354\n",
            "27/27 [==============================] - 0s 9ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        43\n",
            "           1       0.20      1.00      0.33       167\n",
            "           2       0.00      0.00      0.00       139\n",
            "           3       0.00      0.00      0.00       182\n",
            "           4       0.00      0.00      0.00       173\n",
            "           5       0.00      0.00      0.00       149\n",
            "\n",
            "    accuracy                           0.20       853\n",
            "   macro avg       0.03      0.17      0.05       853\n",
            "weighted avg       0.04      0.20      0.06       853\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# For Hybrid Model : Statement + state info\n",
        "# Concatenate 'statement' and 'state info' columns and provide it as input\n",
        "def getHybridXY(data): \n",
        "  y = data['label'].values\n",
        "  X = data['statement'].values+data['state info'].values\n",
        "  for i in range(len(X)):\n",
        "    X[i] = preprocess(X[i])\n",
        "  return X,y\n",
        "\n",
        "  # Preprocessing text data\n",
        "X_train, y_train = getHybridXY(train_df)\n",
        "X_test, y_test = getHybridXY(test_df)\n",
        "\n",
        "# Creating a tokenizer to convert text to sequences\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# Converting text to sequences using the tokenizer\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Padding sequences to a fixed length\n",
        "maxlen = 100\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "# Building the CNN model\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen))\n",
        "model.add(Conv1D(128, 5, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "history = model.fit(X_train, y_train, epochs=10, validation_split=0.2)\n",
        "\n",
        "# Evaluating the model on the test set\n",
        "score, acc = model.evaluate(X_test, y_test)\n",
        "\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)\n",
        "test_preds = model.predict(X_test)\n",
        "print(classification_report(y_test, test_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25RAyv0GCKRk",
        "outputId": "a457e98b-0f21-4ba9-902f-af18a5bead7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "190/190 [==============================] - 13s 63ms/step - loss: -953.2339 - accuracy: 0.1935 - val_loss: -4612.2568 - val_accuracy: 0.1984\n",
            "Epoch 2/10\n",
            "190/190 [==============================] - 11s 57ms/step - loss: -31876.7656 - accuracy: 0.1943 - val_loss: -80221.2656 - val_accuracy: 0.1984\n",
            "Epoch 3/10\n",
            "190/190 [==============================] - 13s 69ms/step - loss: -225913.1719 - accuracy: 0.1943 - val_loss: -413576.8125 - val_accuracy: 0.1984\n",
            "Epoch 4/10\n",
            "190/190 [==============================] - 12s 62ms/step - loss: -836843.0000 - accuracy: 0.1943 - val_loss: -1290745.6250 - val_accuracy: 0.1984\n",
            "Epoch 5/10\n",
            "190/190 [==============================] - 9s 49ms/step - loss: -2190837.7500 - accuracy: 0.1943 - val_loss: -3043496.0000 - val_accuracy: 0.1984\n",
            "Epoch 6/10\n",
            "190/190 [==============================] - 12s 62ms/step - loss: -4656988.5000 - accuracy: 0.1943 - val_loss: -6032833.5000 - val_accuracy: 0.1984\n",
            "Epoch 7/10\n",
            "190/190 [==============================] - 12s 65ms/step - loss: -8647422.0000 - accuracy: 0.1943 - val_loss: -10671977.0000 - val_accuracy: 0.1984\n",
            "Epoch 8/10\n",
            "190/190 [==============================] - 11s 56ms/step - loss: -14544556.0000 - accuracy: 0.1943 - val_loss: -17290342.0000 - val_accuracy: 0.1984\n",
            "Epoch 9/10\n",
            "190/190 [==============================] - 11s 56ms/step - loss: -22772874.0000 - accuracy: 0.1943 - val_loss: -26322332.0000 - val_accuracy: 0.1984\n",
            "Epoch 10/10\n",
            "190/190 [==============================] - 11s 61ms/step - loss: -33734864.0000 - accuracy: 0.1943 - val_loss: -38148848.0000 - val_accuracy: 0.1984\n",
            "27/27 [==============================] - 0s 10ms/step - loss: -41043268.0000 - accuracy: 0.1958\n",
            "Test score: -41043268.0\n",
            "Test accuracy: 0.19577960669994354\n",
            "27/27 [==============================] - 0s 9ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        43\n",
            "           1       0.20      1.00      0.33       167\n",
            "           2       0.00      0.00      0.00       139\n",
            "           3       0.00      0.00      0.00       182\n",
            "           4       0.00      0.00      0.00       173\n",
            "           5       0.00      0.00      0.00       149\n",
            "\n",
            "    accuracy                           0.20       853\n",
            "   macro avg       0.03      0.17      0.05       853\n",
            "weighted avg       0.04      0.20      0.06       853\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# For Hybrid Model : Statement + party affilation\n",
        "# Concatenate 'statement' and 'party affilation' columns and provide it as input\n",
        "def getHybridXY(data): \n",
        "  y = data['label'].values\n",
        "  X = data['statement'].values+data['party affilation'].values\n",
        "  for i in range(len(X)):\n",
        "    X[i] = preprocess(X[i])\n",
        "  return X,y\n",
        "\n",
        "  # Preprocessing text data\n",
        "X_train, y_train = getHybridXY(train_df)\n",
        "X_test, y_test = getHybridXY(test_df)\n",
        "\n",
        "# Creating a tokenizer to convert text to sequences\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# Converting text to sequences using the tokenizer\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Padding sequences to a fixed length\n",
        "maxlen = 100\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "# Building the CNN model\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen))\n",
        "model.add(Conv1D(128, 5, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "history = model.fit(X_train, y_train, epochs=10, validation_split=0.2)\n",
        "\n",
        "# Evaluating the model on the test set\n",
        "score, acc = model.evaluate(X_test, y_test)\n",
        "\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)\n",
        "test_preds = model.predict(X_test)\n",
        "print(classification_report(y_test, test_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_8U1LTdCqXU",
        "outputId": "e8d4ec8d-20b5-4cea-cf7f-e2bf73cc9be2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "190/190 [==============================] - 16s 81ms/step - loss: -1943.7203 - accuracy: 0.1945 - val_loss: -10039.0088 - val_accuracy: 0.1984\n",
            "Epoch 2/10\n",
            "190/190 [==============================] - 11s 57ms/step - loss: -78095.5312 - accuracy: 0.1943 - val_loss: -198714.7031 - val_accuracy: 0.1984\n",
            "Epoch 3/10\n",
            "190/190 [==============================] - 11s 55ms/step - loss: -565913.7500 - accuracy: 0.1943 - val_loss: -1039818.5625 - val_accuracy: 0.1984\n",
            "Epoch 4/10\n",
            "190/190 [==============================] - 12s 64ms/step - loss: -2104802.7500 - accuracy: 0.1943 - val_loss: -3263417.7500 - val_accuracy: 0.1984\n",
            "Epoch 5/10\n",
            "190/190 [==============================] - 12s 62ms/step - loss: -5573928.0000 - accuracy: 0.1943 - val_loss: -7769659.5000 - val_accuracy: 0.1984\n",
            "Epoch 6/10\n",
            "190/190 [==============================] - 10s 50ms/step - loss: -11947067.0000 - accuracy: 0.1943 - val_loss: -15523917.0000 - val_accuracy: 0.1984\n",
            "Epoch 7/10\n",
            "190/190 [==============================] - 12s 65ms/step - loss: -22318338.0000 - accuracy: 0.1943 - val_loss: -27572690.0000 - val_accuracy: 0.1984\n",
            "Epoch 8/10\n",
            "190/190 [==============================] - 12s 65ms/step - loss: -37750536.0000 - accuracy: 0.1943 - val_loss: -44928972.0000 - val_accuracy: 0.1984\n",
            "Epoch 9/10\n",
            "190/190 [==============================] - 11s 57ms/step - loss: -59378160.0000 - accuracy: 0.1943 - val_loss: -68791904.0000 - val_accuracy: 0.1984\n",
            "Epoch 10/10\n",
            "190/190 [==============================] - 11s 56ms/step - loss: -88441704.0000 - accuracy: 0.1943 - val_loss: -100190944.0000 - val_accuracy: 0.1984\n",
            "27/27 [==============================] - 0s 9ms/step - loss: -107741960.0000 - accuracy: 0.1958\n",
            "Test score: -107741960.0\n",
            "Test accuracy: 0.19577960669994354\n",
            "27/27 [==============================] - 0s 9ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        43\n",
            "           1       0.20      1.00      0.33       167\n",
            "           2       0.00      0.00      0.00       139\n",
            "           3       0.00      0.00      0.00       182\n",
            "           4       0.00      0.00      0.00       173\n",
            "           5       0.00      0.00      0.00       149\n",
            "\n",
            "    accuracy                           0.20       853\n",
            "   macro avg       0.03      0.17      0.05       853\n",
            "weighted avg       0.04      0.20      0.06       853\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# For Hybrid Model : Statement + location of statement\n",
        "# Concatenate 'statement' and 'location of statement' columns and provide it as input\n",
        "def getHybridXY(data): \n",
        "  y = data['label'].values\n",
        "  X = data['statement'].values+data['location of statement'].values\n",
        "  for i in range(len(X)):\n",
        "    X[i] = preprocess(X[i])\n",
        "  return X,y\n",
        "\n",
        "  # Preprocessing text data\n",
        "X_train, y_train = getHybridXY(train_df)\n",
        "X_test, y_test = getHybridXY(test_df)\n",
        "\n",
        "# Creating a tokenizer to convert text to sequences\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# Converting text to sequences using the tokenizer\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Padding sequences to a fixed length\n",
        "maxlen = 100\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "# Building the CNN model\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen))\n",
        "model.add(Conv1D(128, 5, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "history = model.fit(X_train, y_train, epochs=10, validation_split=0.2)\n",
        "\n",
        "# Evaluating the model on the test set\n",
        "score, acc = model.evaluate(X_test, y_test)\n",
        "\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)\n",
        "test_preds = model.predict(X_test)\n",
        "print(classification_report(y_test, test_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "RiqsnGgXDFd0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}